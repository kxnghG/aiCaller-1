{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c2f39e7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "SYNONYMS = {\"ag\": \"Ahmed Group\", \"ahmed reit\": \"AGPT\", \"ag trust\": \"AGPT\", \"ag property trust\": \"AGPT\"}\n",
    "\n",
    "def normalize_text_by_synonyms(text: str) -> str:\n",
    "    if not isinstance(text, str):\n",
    "        return text\n",
    "    low = text.lower()\n",
    "    for k in sorted(SYNONYMS.keys(), key=len, reverse=True):\n",
    "        if k in low:\n",
    "            low = low.replace(k, SYNONYMS[k].lower())\n",
    "    return low\n",
    "\n",
    "APPROVED = {\n",
    "    \"company_overview_short\": \"Ahmed Group is a Canadian family office and real-estate firm (est. 1966) focused on discovering and developing purpose-built rental and community-enhancing projects. We manage the full stack: development, asset management, property management, and joint ventures.\",\n",
    "    \"agpt\": \"AGPT is a private real estate investment trust launched Aug 28, 2025, now open to accredited investors.\",\n",
    "    \"dundas_project\": \"A 568-unit mixed-use, purpose-built rental development at 1000 & 1024 Dundas St. E., Mississauga advanced after a May 21, 2024 settlement with Mother Parkers.\",\n",
    "    \"contact_hq\": \"Corporate Head Office: 1-1024 Dundas St. E., Mississauga, ON L4Y 2B8 | (905) 949-0999 | contact@ahmed.group.\"\n",
    "}\n",
    "\n",
    "CMS_QA = {\n",
    "    \"about\": APPROVED[\"company_overview_short\"],\n",
    "    \"location\": APPROVED[\"contact_hq\"],\n",
    "    \"dundas\": APPROVED[\"dundas_project\"],\n",
    "    \"vendor\": \"Review our Vendor Guidelines and Tenders/Bidding pages; I can also collect your details for Procurement.\",\n",
    "    \"agpt\": \"I can connect you to our Investor Relations team and share an AGPT overview. Please confirm your accreditation and best email.\"\n",
    "}\n",
    "\n",
    "INVESTMENT_DISCLAIMER = \"The information provided about AGPT is for informational purposes only and does not constitute an offer to sell or a solicitation to buy any securities. Offers are made only by official offering documents to qualified (accredited) investors and are subject to applicable securities laws. Past performance is not indicative of future results.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "02d81df0",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mos\u001b[39;00m,\u001b[38;5;250m \u001b[39m\u001b[34;01mtime\u001b[39;00m,\u001b[38;5;250m \u001b[39m\u001b[34;01msubprocess\u001b[39;00m,\u001b[38;5;250m \u001b[39m\u001b[34;01mre\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnp\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msounddevice\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msd\u001b[39;00m\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mwebrtcvad\u001b[39;00m\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/GenieAI/AIcallerV2/.venv/lib64/python3.11/site-packages/sounddevice.py:2957\u001b[39m\n\u001b[32m   2953\u001b[39m         _terminate()\n\u001b[32m   2956\u001b[39m _atexit.register(_exit_handler)\n\u001b[32m-> \u001b[39m\u001b[32m2957\u001b[39m \u001b[43m_initialize\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2959\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[34m__name__\u001b[39m == \u001b[33m'\u001b[39m\u001b[33m__main__\u001b[39m\u001b[33m'\u001b[39m:\n\u001b[32m   2960\u001b[39m     \u001b[38;5;28mprint\u001b[39m(query_devices())\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/GenieAI/AIcallerV2/.venv/lib64/python3.11/site-packages/sounddevice.py:2921\u001b[39m, in \u001b[36m_initialize\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m   2919\u001b[39m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[32m   2920\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m2921\u001b[39m     _check(\u001b[43m_lib\u001b[49m\u001b[43m.\u001b[49m\u001b[43mPa_Initialize\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m, \u001b[33m'\u001b[39m\u001b[33mError initializing PortAudio\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m   2922\u001b[39m     \u001b[38;5;28;01mglobal\u001b[39;00m _initialized\n\u001b[32m   2923\u001b[39m     _initialized += \u001b[32m1\u001b[39m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "import os, time, subprocess, re\n",
    "import numpy as np\n",
    "import sounddevice as sd\n",
    "import webrtcvad\n",
    "import torch\n",
    "from dotenv import load_dotenv\n",
    "from collections import deque\n",
    "from faster_whisper import WhisperModel\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "openai_api_key = os.environ.get(\"OPENAI_API_KEY\")\n",
    "USE_GPU_WHISPER = os.environ.get(\"USE_GPU_WHISPER\", \"0\") == \"1\"\n",
    "GPU_COMPUTE_TYPE = os.environ.get(\"GPU_COMPUTE_TYPE\", \"float16\")\n",
    "PIPER_VOICE = os.environ.get(\"PIPER_VOICE\", \"./en_US-amy-medium.onnx\")\n",
    "PIPER_BIN = os.environ.get(\"PIPER_BIN\", \"./piper/piper\")\n",
    "\n",
    "SAMPLE_RATE = 16000\n",
    "FRAME_MS = 10\n",
    "UTTER_TIMEOUT = 0.4\n",
    "MIN_UTTER_SEC = 0.5\n",
    "MAX_AUDIO_BUFFER_SEC = 10\n",
    "\n",
    "SYSTEM_PROMPT = \"You are Ahmed Group Concierge. Be professional and brief (1-2 sentences). Only provide contact details (phone, email, address) if the user explicitly asks for contact information or how to reach Ahmed Group. Otherwise, do not include contact info.\"\n",
    "\n",
    "SEED_SNIPPETS = [\n",
    "    {\"id\": \"about-001\", \"title\": \"Company Overview\", \"text\": \"Ahmed Group is a Canadian real-estate firm and family office (est. 1966) focused on purpose-built rental and community-enhancing projects.\"},\n",
    "    {\"id\": \"globe-2025-08-28\", \"title\": \"AG Property Trust\", \"text\": \"AGPT is a private real estate investment trust launched Aug 28, 2025, now open to accredited investors.\"},\n",
    "    {\"id\": \"dundas-2024-05-21\", \"title\": \"Dundas Settlement\", \"text\": \"A 568-unit mixed-use rental at 1000 & 1024 Dundas St. E., Mississauga.\"},\n",
    "    {\"id\": \"contact-001\", \"title\": \"Contact\", \"text\": \"1-1024 Dundas St. E., Mississauga, ON | (905) 949-0999 | contact@ahmed.group.\"}\n",
    "]\n",
    "\n",
    "CONTACT_KEYWORDS = [\"contact\", \"phone\", \"email\", \"reach\", \"call\", \"address\", \"location\", \"where are you\", \"hq\", \"head office\"]\n",
    "FAREWELL_KEYWORDS = [\"thank you that's all\", \"thanks that's all\", \"that's all thank you\", \"that's all thanks\", \"that's everything\", \"that'll be all\", \"that will be all\", \"i'm good thank you\", \"i'm all set\", \"that's it thank you\", \"that's it thanks\"]\n",
    "\n",
    "EMAIL_RE = re.compile(r\"\\b[\\w\\.-]+@[\\w\\.-]+\\.[A-Za-z]{2,}\\b\")\n",
    "PHONE_RE = re.compile(r\"\\+?\\d[\\d\\s\\-()]{7,}\\d\")\n",
    "ADDRESS_HINTS = [\"dundas st\", \"l4y\", \"mississauga\", \"1-1024 dundas\"]\n",
    "\n",
    "def contact_intent(text: str) -> bool:\n",
    "    t = (text or \"\").lower()\n",
    "    return any(k in t for k in CONTACT_KEYWORDS)\n",
    "\n",
    "def farewell_intent(text: str) -> bool:\n",
    "    t = (text or \"\").lower().strip()\n",
    "    if any(phrase in t for phrase in FAREWELL_KEYWORDS):\n",
    "        return True\n",
    "    if len(t.split()) <= 6 and \"thank\" in t and \"all\" in t:\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "def contains_contact_info(s: str) -> bool:\n",
    "    ss = s.lower()\n",
    "    return EMAIL_RE.search(s) or PHONE_RE.search(s) or any(h in ss for h in ADDRESS_HINTS)\n",
    "\n",
    "class RecentDeduper:\n",
    "    def __init__(self, max_items: int = 64):\n",
    "        self.buf = deque()\n",
    "        self.max_items = max_items\n",
    "\n",
    "    def _norm(self, s: str) -> str:\n",
    "        return normalize_for_dedupe(s)\n",
    "\n",
    "    def seen(self, s: str, within: float = 8.0) -> bool:\n",
    "        now = time.time()\n",
    "        key = self._norm(s)\n",
    "        while self.buf and (now - self.buf[0][1]) > within:\n",
    "            self.buf.popleft()\n",
    "        for txt, ts in self.buf:\n",
    "            if txt == key:\n",
    "                return True\n",
    "        self.buf.append((key, now))\n",
    "        if len(self.buf) > self.max_items:\n",
    "            self.buf.popleft()\n",
    "        return False\n",
    "\n",
    "def normalize_for_dedupe(s: str) -> str:\n",
    "    if not isinstance(s, str):\n",
    "        return \"\"\n",
    "    t = s.strip().lower()\n",
    "    t = t.replace(\"\\u2013\", \"-\").replace(\"\\u2014\", \"-\")\n",
    "    t = re.sub(r\"\\s+\", \" \", t)\n",
    "    t = re.sub(r\"[^a-z0-9 ]+\", \"\", t)\n",
    "    return t.strip()\n",
    "\n",
    "class VAD:\n",
    "    def __init__(self):\n",
    "        self.vad = webrtcvad.Vad(1)\n",
    "        self.sr = SAMPLE_RATE\n",
    "        self.noise_gate = 300\n",
    "\n",
    "    def is_speech(self, pcm16: np.ndarray) -> bool:\n",
    "        if np.abs(pcm16).max() < self.noise_gate:\n",
    "            return False\n",
    "        try:\n",
    "            return self.vad.is_speech(pcm16.tobytes(), self.sr)\n",
    "        except:\n",
    "            return False\n",
    "\n",
    "def kw_rag(query: str, k: int = 2):\n",
    "    q = query.lower().split()\n",
    "    allow_contact = contact_intent(query)\n",
    "    docs = list(SEED_SNIPPETS)\n",
    "    if \"APPROVED\" in globals():\n",
    "        docs.extend([\n",
    "            {\"id\": \"approved-about\", \"title\": \"Company Overview (Approved)\", \"text\": APPROVED.get(\"company_overview_short\", \"\")},\n",
    "            {\"id\": \"approved-agpt\", \"title\": \"AGPT (Approved)\", \"text\": APPROVED.get(\"agpt\", \"\")},\n",
    "            {\"id\": \"approved-dundas\", \"title\": \"Dundas Project (Approved)\", \"text\": APPROVED.get(\"dundas_project\", \"\")},\n",
    "            {\"id\": \"approved-contact\", \"title\": \"Contact (Approved)\", \"text\": APPROVED.get(\"contact_hq\", \"\")}\n",
    "        ])\n",
    "    if not allow_contact:\n",
    "        docs = [d for d in docs if not (\"contact\" in d.get(\"title\", \"\").lower() or \"contact\" in str(d.get(\"id\", \"\")).lower())]\n",
    "    scored = [(sum(w in d[\"text\"].lower() for w in q), i, d) for i, d in enumerate(docs)]\n",
    "    scored.sort(reverse=True, key=lambda x: x[0])\n",
    "    return [{\"title\": d[\"title\"], \"text\": d[\"text\"][:200], \"docId\": d.get(\"id\", str(i))} for _, i, d in scored[:k]]\n",
    "\n",
    "def cms_match(user_text: str):\n",
    "    if \"CMS_QA\" not in globals():\n",
    "        return None\n",
    "    text = user_text if isinstance(user_text, str) else \"\"\n",
    "    norm = text.lower()\n",
    "    if \"normalize_text_by_synonyms\" in globals():\n",
    "        try:\n",
    "            norm = normalize_text_by_synonyms(text)\n",
    "        except:\n",
    "            norm = text.lower()\n",
    "\n",
    "    def any_in(words):\n",
    "        return any(w in norm for w in words)\n",
    "\n",
    "    if farewell_intent(text):\n",
    "        return \"Thank you for contacting Ahmed Group. Feel free to reach out anytime—we're here to help!\"\n",
    "    if any_in([\"agpt\", \"accredited investor\", \"accredited\", \"ag property trust\", \"ag trust\", \"ahmed reit\"]):\n",
    "        base = CMS_QA.get(\"agpt\")\n",
    "        if not base:\n",
    "            return None\n",
    "        if \"INVESTMENT_DISCLAIMER\" in globals():\n",
    "            return base + \" \" + INVESTMENT_DISCLAIMER\n",
    "        return base\n",
    "    if any_in([\"dundas\", \"1000\", \"1024\", \"mother parkers\", \"dundas st. e.\"]):\n",
    "        return CMS_QA.get(\"dundas\")\n",
    "    if any_in([\"vendor\", \"procure\", \"procurement\", \"tender\", \"bid\", \"bidding\"]):\n",
    "        return CMS_QA.get(\"vendor\")\n",
    "    if any_in(CONTACT_KEYWORDS):\n",
    "        return CMS_QA.get(\"location\")\n",
    "    if any_in([\"what do you do\", \"about\", \"company\", \"what is ahmed\", \"who are you\", \"overview\"]):\n",
    "        return CMS_QA.get(\"about\")\n",
    "    return None\n",
    "\n",
    "ASR = None\n",
    "\n",
    "def init_asr():\n",
    "    global ASR\n",
    "    if USE_GPU_WHISPER and torch.cuda.is_available():\n",
    "        try:\n",
    "            print(f\"Loading Whisper tiny.en on GPU ({GPU_COMPUTE_TYPE})...\")\n",
    "            ASR = WhisperModel(\"tiny.en\", device=\"cuda\", compute_type=GPU_COMPUTE_TYPE, num_workers=1)\n",
    "            print(\"✓ ASR ready on GPU\")\n",
    "            return\n",
    "        except Exception as e:\n",
    "            print(f\"GPU init failed ({e}); falling back to CPU int8\")\n",
    "    print(\"Loading Whisper tiny.en on CPU (int8)...\")\n",
    "    ASR = WhisperModel(\"tiny.en\", device=\"cpu\", compute_type=\"int8\", num_workers=2)\n",
    "\n",
    "def transcribe(pcm16: np.ndarray) -> str:\n",
    "    audio = pcm16.astype(np.float32) / 32768.0\n",
    "    segments, _ = ASR.transcribe(audio, language=\"en\", beam_size=1)\n",
    "    return \" \".join(s.text.strip() for s in segments).strip()\n",
    "\n",
    "def llm_generate(user_text: str, snippets) -> str:\n",
    "    from openai import OpenAI\n",
    "    client = OpenAI()\n",
    "    ctx = \"\\n\".join(f\"- {s['title']}: {s['text']}\" for s in snippets)\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": f\"{SYSTEM_PROMPT}\\n\\nUse the following context to answer the user's question:\\n{ctx}\"},\n",
    "        {\"role\": \"user\", \"content\": user_text}\n",
    "    ]\n",
    "    response = client.chat.completions.create(model=\"gpt-3.5-turbo\", messages=messages, temperature=0.3, max_tokens=150)\n",
    "    return response.choices[0].message.content.strip()\n",
    "\n",
    "def llm_stream_sentences(user_text: str, snippets):\n",
    "    from openai import OpenAI\n",
    "    client = OpenAI()\n",
    "    ctx = \"\\n\".join(f\"- {s['title']}: {s['text']}\" for s in snippets)\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": f\"{SYSTEM_PROMPT}\\n\\nUse the following context to answer the user's question:\\n{ctx}\"},\n",
    "        {\"role\": \"user\", \"content\": user_text}\n",
    "    ]\n",
    "    buffer = \"\"\n",
    "    try:\n",
    "        stream = client.chat.completions.create(model=\"gpt-3.5-turbo\", messages=messages, temperature=0.3, max_tokens=150, stream=True)\n",
    "        for chunk in stream:\n",
    "            try:\n",
    "                token = getattr(getattr(chunk.choices[0], \"delta\", None), \"content\", None)\n",
    "            except:\n",
    "                token = None\n",
    "            if not token:\n",
    "                continue\n",
    "            buffer += token\n",
    "            parts = re.split(r\"(?<=[.!?])\\s+\", buffer)\n",
    "            for sent in parts[:-1]:\n",
    "                s = sent.strip()\n",
    "                if s:\n",
    "                    yield s\n",
    "            buffer = parts[-1]\n",
    "        if buffer.strip():\n",
    "            yield buffer.strip()\n",
    "    except:\n",
    "        yield llm_generate(user_text, snippets)\n",
    "\n",
    "def piper_tts(text: str):\n",
    "    if not os.path.exists(PIPER_BIN) or not os.path.exists(PIPER_VOICE):\n",
    "        return np.zeros(0, dtype=np.int16), SAMPLE_RATE\n",
    "    env = os.environ.copy()\n",
    "    env['LD_LIBRARY_PATH'] = f\"{os.path.dirname(PIPER_BIN)}:{env.get('LD_LIBRARY_PATH', '')}\"\n",
    "    proc = subprocess.Popen([PIPER_BIN, \"--model\", PIPER_VOICE, \"--sentence_silence\", \"0.08\", \"--length_scale\", \"0.8\", \"--output-raw\"],\n",
    "                           stdin=subprocess.PIPE, stdout=subprocess.PIPE, stderr=subprocess.DEVNULL, env=env)\n",
    "    pcm_bytes, _ = proc.communicate(input=text.encode(\"utf-8\"), timeout=20)\n",
    "    pcm = np.frombuffer(pcm_bytes, dtype=np.int16)\n",
    "    return pcm, 22050\n",
    "\n",
    "def sentence_split(text: str):\n",
    "    return [s.strip() for s in re.split(r\"(?<=[.!?])\\s+\", text or \"\") if s.strip()]\n",
    "\n",
    "def speak_sentences(sentences, allow_contact: bool, dedupe_ai: RecentDeduper, ioa):\n",
    "    spoken_keys = set()\n",
    "    output_sentences = []\n",
    "    for sent in sentences:\n",
    "        if not sent:\n",
    "            continue\n",
    "        if not allow_contact and contains_contact_info(sent):\n",
    "            continue\n",
    "        key = normalize_for_dedupe(sent)\n",
    "        if not key or key in spoken_keys:\n",
    "            continue\n",
    "        if dedupe_ai.seen(sent, within=10.0):\n",
    "            continue\n",
    "        spoken_keys.add(key)\n",
    "        output_sentences.append(sent)\n",
    "    if output_sentences:\n",
    "        print(f\"AI: {' '.join(output_sentences)}\")\n",
    "        for sent in output_sentences:\n",
    "            audio, sr = piper_tts(sent)\n",
    "            ioa.play(audio, sr)\n",
    "\n",
    "def process_text(text: str, ioa, dedupe_ai: RecentDeduper):\n",
    "    allow_contact = contact_intent(text)\n",
    "    cms = cms_match(text)\n",
    "    if cms:\n",
    "        return speak_sentences(sentence_split(cms), allow_contact, dedupe_ai, ioa)\n",
    "    snippets = kw_rag(text)\n",
    "    return speak_sentences(llm_stream_sentences(text, snippets), allow_contact, dedupe_ai, ioa)\n",
    "\n",
    "class AudioIO:\n",
    "    def __init__(self):\n",
    "        self.sr = SAMPLE_RATE\n",
    "    \n",
    "    def mic_frames(self):\n",
    "        frames = int(self.sr * FRAME_MS / 1000)\n",
    "        with sd.InputStream(samplerate=self.sr, channels=1, dtype='int16', blocksize=frames) as stream:\n",
    "            while True:\n",
    "                data, _ = stream.read(frames)\n",
    "                yield data.flatten()\n",
    "    \n",
    "    def play(self, pcm16: np.ndarray, sr: int | None = None):\n",
    "        if pcm16.size == 0:\n",
    "            return\n",
    "        sd.play(pcm16, sr if sr else self.sr)\n",
    "        sd.wait()\n",
    "\n",
    "def main():\n",
    "    init_asr()\n",
    "    ioa = AudioIO()\n",
    "    vad = VAD()\n",
    "    dedupe_user = RecentDeduper()\n",
    "    dedupe_ai = RecentDeduper()\n",
    "    \n",
    "    greeting = \"Welcome to Ahmed Group. How can I assist you today?\"\n",
    "    print(f\"AI: {greeting}\")\n",
    "    audio, sr = piper_tts(greeting)\n",
    "    ioa.play(audio, sr)\n",
    "    \n",
    "    buf = []\n",
    "    last_speech = time.time()\n",
    "\n",
    "    try:\n",
    "        for frame in ioa.mic_frames():\n",
    "            if vad.is_speech(frame):\n",
    "                buf.append(frame)\n",
    "                last_speech = time.time()\n",
    "                continue\n",
    "            \n",
    "            if buf and (time.time() - last_speech) > UTTER_TIMEOUT:\n",
    "                pcm = np.concatenate(buf)\n",
    "                buf = []\n",
    "                duration = len(pcm) / SAMPLE_RATE\n",
    "                if duration < MIN_UTTER_SEC or duration > MAX_AUDIO_BUFFER_SEC:\n",
    "                    continue\n",
    "                text = transcribe(pcm)\n",
    "                if not text:\n",
    "                    continue\n",
    "                if dedupe_user.seen(text, within=6.0):\n",
    "                    continue\n",
    "                print(f\"User: {text}\")\n",
    "                process_text(text, ioa, dedupe_ai)\n",
    "    except KeyboardInterrupt:\n",
    "        print(\"\\nStopped\")\n",
    "\n",
    "if globals().get(\"_VOICE_CONCIERGE_RUNNING\"):\n",
    "    print(\"(Already running — ignoring duplicate start)\")\n",
    "else:\n",
    "    globals()[\"_VOICE_CONCIERGE_RUNNING\"] = True\n",
    "    try:\n",
    "        main()\n",
    "    finally:\n",
    "        globals()[\"_VOICE_CONCIERGE_RUNNING\"] = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "018bf1c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[RAG] Loaded existing index with 860 vectors.\n",
      "Loading Whisper tiny.en on GPU (float16)...\n",
      "✓ ASR ready on GPU\n",
      "AI: Welcome to Ahmed Group. How can I assist you today?\n",
      "✓ ASR ready on GPU\n",
      "AI: Welcome to Ahmed Group. How can I assist you today?\n"
     ]
    },
    {
     "ename": "AuthenticationError",
     "evalue": "Error code: 401 - {'error': {'message': 'Incorrect API key provided: sk-proj-**************************************************************************************************7Zu-. You can find your API key at https://platform.openai.com/account/api-keys.', 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_api_key'}}",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAuthenticationError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 479\u001b[39m\n\u001b[32m    469\u001b[39m     pdfs = [\n\u001b[32m    470\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m/home/kxngh/Desktop/GenieAI/AIcallerV2/kb/Exempt Market Proficiency Course-EMP.pdf\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    471\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m/home/kxngh/Desktop/GenieAI/AIcallerV2/kb/Tawakkul Fund Deck.pdf\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    472\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m/home/kxngh/Desktop/GenieAI/AIcallerV2/kb/Unregistered Principal Training Program.pdf\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    473\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mkb/v5.3 - AGPT Deck.pdf\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    474\u001b[39m     ]\n\u001b[32m    475\u001b[39m     extra_blobs = [\n\u001b[32m    476\u001b[39m         \u001b[38;5;66;03m# \"Short policy note ...\",\u001b[39;00m\n\u001b[32m    477\u001b[39m         \u001b[38;5;66;03m# \"FAQ entry ...\",\u001b[39;00m\n\u001b[32m    478\u001b[39m     ]\n\u001b[32m--> \u001b[39m\u001b[32m479\u001b[39m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpdf_paths\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpdfs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_texts\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_blobs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mforce_rebuild\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m    480\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    481\u001b[39m     \u001b[38;5;28mglobals\u001b[39m()[\u001b[33m\"\u001b[39m\u001b[33m_VOICE_CONCIERGE_RUNNING\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 433\u001b[39m, in \u001b[36mmain\u001b[39m\u001b[34m(pdf_paths, extra_texts, force_rebuild)\u001b[39m\n\u001b[32m    431\u001b[39m greeting = \u001b[33m\"\u001b[39m\u001b[33mWelcome to Ahmed Group. How can I assist you today?\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    432\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mAI: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mgreeting\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m433\u001b[39m g_pcm, g_sr = \u001b[43mtts_openai\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgreeting\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    434\u001b[39m ioa.play(g_pcm, g_sr)\n\u001b[32m    436\u001b[39m buf = []\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 323\u001b[39m, in \u001b[36mtts_openai\u001b[39m\u001b[34m(text)\u001b[39m\n\u001b[32m    321\u001b[39m text = soften_punctuation(text)\n\u001b[32m    322\u001b[39m \u001b[38;5;66;03m# Non-streaming is simpler and yields fewer artifacts for short prompts\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m323\u001b[39m audio = \u001b[43mclient\u001b[49m\u001b[43m.\u001b[49m\u001b[43maudio\u001b[49m\u001b[43m.\u001b[49m\u001b[43mspeech\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    324\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mOPENAI_TTS_MODEL\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    325\u001b[39m \u001b[43m    \u001b[49m\u001b[43mvoice\u001b[49m\u001b[43m=\u001b[49m\u001b[43mVOICE_NAME\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    326\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m=\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    327\u001b[39m \u001b[43m    \u001b[49m\u001b[43mresponse_format\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mwav\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m   \u001b[49m\u001b[38;5;66;43;03m# <---- IMPORTANT: not 'format'\u001b[39;49;00m\n\u001b[32m    328\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    329\u001b[39m wav_bytes = audio.content \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(audio, \u001b[33m\"\u001b[39m\u001b[33mcontent\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m audio  \u001b[38;5;66;03m# SDK returns a Response with .content\u001b[39;00m\n\u001b[32m    330\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(wav_bytes, \u001b[38;5;28mbytes\u001b[39m) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m:\n\u001b[32m    331\u001b[39m     \u001b[38;5;66;03m# Some SDK versions return a dict-like; get .content_bytes()\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/GenieAI/AIcallerV2/.venv/lib64/python3.11/site-packages/openai/resources/audio/speech.py:103\u001b[39m, in \u001b[36mSpeech.create\u001b[39m\u001b[34m(self, input, model, voice, instructions, response_format, speed, stream_format, extra_headers, extra_query, extra_body, timeout)\u001b[39m\n\u001b[32m     67\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     68\u001b[39m \u001b[33;03mGenerates audio from the input text.\u001b[39;00m\n\u001b[32m     69\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m    100\u001b[39m \u001b[33;03m  timeout: Override the client-level default timeout for this request, in seconds\u001b[39;00m\n\u001b[32m    101\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    102\u001b[39m extra_headers = {\u001b[33m\"\u001b[39m\u001b[33mAccept\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33mapplication/octet-stream\u001b[39m\u001b[33m\"\u001b[39m, **(extra_headers \u001b[38;5;129;01mor\u001b[39;00m {})}\n\u001b[32m--> \u001b[39m\u001b[32m103\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_post\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    104\u001b[39m \u001b[43m    \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m/audio/speech\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    105\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbody\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmaybe_transform\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    106\u001b[39m \u001b[43m        \u001b[49m\u001b[43m{\u001b[49m\n\u001b[32m    107\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43minput\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    108\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmodel\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    109\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mvoice\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mvoice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    110\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43minstructions\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43minstructions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    111\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mresponse_format\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mresponse_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    112\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mspeed\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mspeed\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    113\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstream_format\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    114\u001b[39m \u001b[43m        \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    115\u001b[39m \u001b[43m        \u001b[49m\u001b[43mspeech_create_params\u001b[49m\u001b[43m.\u001b[49m\u001b[43mSpeechCreateParams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    116\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    117\u001b[39m \u001b[43m    \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmake_request_options\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    118\u001b[39m \u001b[43m        \u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_query\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_query\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_body\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_body\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\n\u001b[32m    119\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    120\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m=\u001b[49m\u001b[43m_legacy_response\u001b[49m\u001b[43m.\u001b[49m\u001b[43mHttpxBinaryResponseContent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    121\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/GenieAI/AIcallerV2/.venv/lib64/python3.11/site-packages/openai/_base_client.py:1259\u001b[39m, in \u001b[36mSyncAPIClient.post\u001b[39m\u001b[34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[39m\n\u001b[32m   1245\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mpost\u001b[39m(\n\u001b[32m   1246\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   1247\u001b[39m     path: \u001b[38;5;28mstr\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1254\u001b[39m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_StreamT] | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   1255\u001b[39m ) -> ResponseT | _StreamT:\n\u001b[32m   1256\u001b[39m     opts = FinalRequestOptions.construct(\n\u001b[32m   1257\u001b[39m         method=\u001b[33m\"\u001b[39m\u001b[33mpost\u001b[39m\u001b[33m\"\u001b[39m, url=path, json_data=body, files=to_httpx_files(files), **options\n\u001b[32m   1258\u001b[39m     )\n\u001b[32m-> \u001b[39m\u001b[32m1259\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(ResponseT, \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/GenieAI/AIcallerV2/.venv/lib64/python3.11/site-packages/openai/_base_client.py:1047\u001b[39m, in \u001b[36mSyncAPIClient.request\u001b[39m\u001b[34m(self, cast_to, options, stream, stream_cls)\u001b[39m\n\u001b[32m   1044\u001b[39m             err.response.read()\n\u001b[32m   1046\u001b[39m         log.debug(\u001b[33m\"\u001b[39m\u001b[33mRe-raising status error\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m1047\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m._make_status_error_from_response(err.response) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1049\u001b[39m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[32m   1051\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m response \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[33m\"\u001b[39m\u001b[33mcould not resolve response (should never happen)\u001b[39m\u001b[33m\"\u001b[39m\n",
      "\u001b[31mAuthenticationError\u001b[39m: Error code: 401 - {'error': {'message': 'Incorrect API key provided: sk-proj-**************************************************************************************************7Zu-. You can find your API key at https://platform.openai.com/account/api-keys.', 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_api_key'}}"
     ]
    }
   ],
   "source": [
    "import os, io, time, subprocess, re, math, wave, struct, pathlib, json\n",
    "import numpy as np\n",
    "import sounddevice as sd\n",
    "import webrtcvad\n",
    "import torch\n",
    "from dotenv import load_dotenv\n",
    "from collections import deque\n",
    "from faster_whisper import WhisperModel\n",
    "\n",
    "# --------- Setup & Env ----------\n",
    "load_dotenv()\n",
    "\n",
    "openai_api_key = os.environ.get(\"OPENAI_API_KEY\")\n",
    "USE_GPU_WHISPER = os.environ.get(\"USE_GPU_WHISPER\", \"0\") == \"1\"\n",
    "GPU_COMPUTE_TYPE = os.environ.get(\"GPU_COMPUTE_TYPE\", \"float16\")\n",
    "\n",
    "OPENAI_TTS_MODEL = os.environ.get(\"OPENAI_TTS_MODEL\", \"gpt-4o-mini-tts\")\n",
    "VOICE_NAME = os.environ.get(\"VOICE_NAME\", \"alloy\")\n",
    "\n",
    "EMBED_MODEL = os.environ.get(\"EMBED_MODEL\", \"text-embedding-3-small\")\n",
    "INDEX_PATH = os.environ.get(\"RAG_INDEX_PATH\", \"./rag_index.faiss\")\n",
    "INDEX_META_PATH = os.environ.get(\"RAG_META_PATH\", \"./rag_meta.json\")\n",
    "\n",
    "SAMPLE_RATE = 16000\n",
    "FRAME_MS = 10\n",
    "UTTER_TIMEOUT = 0.40\n",
    "MIN_UTTER_SEC = 0.50\n",
    "MAX_AUDIO_BUFFER_SEC = 10.0\n",
    "\n",
    "SYSTEM_PROMPT = (\n",
    "    \"You are Ahmed Group Concierge. Be professional and brief (1-2 sentences). \"\n",
    "    \"Only provide contact details (phone, email, address) if the user explicitly \"\n",
    "    \"asks for contact information or how to reach Ahmed Group. Otherwise, do not include contact info.\"\n",
    ")\n",
    "\n",
    "SEED_SNIPPETS = [\n",
    "    {\"id\": \"about-001\", \"title\": \"Company Overview\", \"text\": \"Ahmed Group is a Canadian real-estate firm and family office (est. 1966) focused on purpose-built rental and community-enhancing projects.\"},\n",
    "    {\"id\": \"globe-2025-08-28\", \"title\": \"AG Property Trust\", \"text\": \"AGPT is a private real estate investment trust launched Aug 28, 2025, now open to accredited investors.\"},\n",
    "    {\"id\": \"dundas-2024-05-21\", \"title\": \"Dundas Settlement\", \"text\": \"A 568-unit mixed-use rental at 1000 & 1024 Dundas St. E., Mississauga.\"},\n",
    "    {\"id\": \"contact-001\", \"title\": \"Contact\", \"text\": \"1-1024 Dundas St. E., Mississauga, ON | (905) 949-0999 | contact@ahmed.group.\"}\n",
    "]\n",
    "\n",
    "CONTACT_KEYWORDS = [\"contact\", \"phone\", \"email\", \"reach\", \"call\", \"address\", \"location\", \"where are you\", \"hq\", \"head office\"]\n",
    "FAREWELL_KEYWORDS = [\n",
    "    \"thank you that's all\",\"thanks that's all\",\"that's all thank you\",\"that's all thanks\",\n",
    "    \"that's everything\",\"that'll be all\",\"that will be all\",\"i'm good thank you\",\n",
    "    \"i'm all set\",\"that's it thank you\",\"that's it thanks\"\n",
    "]\n",
    "\n",
    "EMAIL_RE = re.compile(r\"\\b[\\w\\.-]+@[\\w\\.-]+\\.[A-Za-z]{2,}\\b\")\n",
    "PHONE_RE = re.compile(r\"\\+?\\d[\\d\\s\\-()]{7,}\\d\")\n",
    "ADDRESS_HINTS = [\"dundas st\", \"l4y\", \"mississauga\", \"1-1024 dundas\"]\n",
    "\n",
    "# --------- OpenAI client helper ----------\n",
    "def get_openai():\n",
    "    from openai import OpenAI\n",
    "    key = os.environ.get(\"OPENAI_API_KEY\") or openai_api_key\n",
    "    if not key:\n",
    "        raise RuntimeError(\"OPENAI_API_KEY not set\")\n",
    "    return OpenAI(api_key=key)\n",
    "\n",
    "# --------- Anti-repeat helpers ----------\n",
    "def normalize_for_dedupe(s: str) -> str:\n",
    "    if not isinstance(s, str):\n",
    "        return \"\"\n",
    "    t = s.strip().lower()\n",
    "    t = t.replace(\"\\u2013\", \"-\").replace(\"\\u2014\", \"-\")\n",
    "    t = re.sub(r\"\\s+\", \" \", t)\n",
    "    t = re.sub(r\"[^a-z0-9 ]+\", \"\", t)\n",
    "    return t.strip()\n",
    "\n",
    "class RecentDeduper:\n",
    "    def __init__(self, max_items: int = 64):\n",
    "        from collections import deque\n",
    "        self.buf = deque()\n",
    "        self.max_items = max_items\n",
    "    def seen(self, s: str, within: float = 8.0) -> bool:\n",
    "        now = time.time()\n",
    "        key = normalize_for_dedupe(s)\n",
    "        while self.buf and (now - self.buf[0][1]) > within:\n",
    "            self.buf.popleft()\n",
    "        for txt, ts in self.buf:\n",
    "            if txt == key:\n",
    "                return True\n",
    "        self.buf.append((key, now))\n",
    "        if len(self.buf) > self.max_items:\n",
    "            self.buf.popleft()\n",
    "        return False\n",
    "\n",
    "# --------- Intent helpers ----------\n",
    "def contact_intent(text: str) -> bool:\n",
    "    t = (text or \"\").lower()\n",
    "    return any(k in t for k in CONTACT_KEYWORDS)\n",
    "\n",
    "def farewell_intent(text: str) -> bool:\n",
    "    t = (text or \"\").lower().strip()\n",
    "    if any(phrase in t for phrase in FAREWELL_KEYWORDS):\n",
    "        return True\n",
    "    if len(t.split()) <= 6 and \"thank\" in t and \"all\" in t:\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "def contains_contact_info(s: str) -> bool:\n",
    "    ss = s.lower()\n",
    "    return EMAIL_RE.search(s) or PHONE_RE.search(s) or any(h in ss for h in ADDRESS_HINTS)\n",
    "\n",
    "# --------- VAD ----------\n",
    "class VAD:\n",
    "    def __init__(self):\n",
    "        self.vad = webrtcvad.Vad(1)\n",
    "        self.sr = SAMPLE_RATE\n",
    "        self.noise_gate = 300\n",
    "    def is_speech(self, pcm16: np.ndarray) -> bool:\n",
    "        if np.abs(pcm16).max() < self.noise_gate:\n",
    "            return False\n",
    "        try:\n",
    "            return self.vad.is_speech(pcm16.tobytes(), self.sr)\n",
    "        except:\n",
    "            return False\n",
    "\n",
    "# --------- Whisper ASR ----------\n",
    "ASR = None\n",
    "def init_asr():\n",
    "    global ASR\n",
    "    if USE_GPU_WHISPER and torch.cuda.is_available():\n",
    "        try:\n",
    "            print(f\"Loading Whisper tiny.en on GPU ({GPU_COMPUTE_TYPE})...\")\n",
    "            ASR = WhisperModel(\"tiny.en\", device=\"cuda\", compute_type=GPU_COMPUTE_TYPE, num_workers=1)\n",
    "            print(\"✓ ASR ready on GPU\")\n",
    "            return\n",
    "        except Exception as e:\n",
    "            print(f\"GPU init failed ({e}); falling back to CPU int8\")\n",
    "    print(\"Loading Whisper tiny.en on CPU (int8)...\")\n",
    "    ASR = WhisperModel(\"tiny.en\", device=\"cpu\", compute_type=\"int8\", num_workers=2)\n",
    "\n",
    "def transcribe(pcm16: np.ndarray) -> str:\n",
    "    audio = pcm16.astype(np.float32) / 32768.0\n",
    "    segments, _ = ASR.transcribe(audio, language=\"en\", beam_size=1)\n",
    "    return \" \".join(s.text.strip() for s in segments).strip()\n",
    "\n",
    "# --------- Simple seed-RAG ----------\n",
    "def kw_rag(query: str, k: int = 2):\n",
    "    q = query.lower().split()\n",
    "    allow_contact = contact_intent(query)\n",
    "    docs = list(SEED_SNIPPETS)\n",
    "    # merge vector-RAG results later as well\n",
    "    scored = [(sum(w in d[\"text\"].lower() for w in q), i, d) for i, d in enumerate(docs)]\n",
    "    scored.sort(reverse=True, key=lambda x: x[0])\n",
    "    out = [{\"title\": d[\"title\"], \"text\": d[\"text\"][:200], \"docId\": d.get(\"id\", str(i))} for _, i, d in scored[:k]]\n",
    "    if not allow_contact:\n",
    "        out = [d for d in out if \"contact\" not in d[\"title\"].lower()]\n",
    "    return out\n",
    "\n",
    "# --------- Vector RAG over PDFs & extra text ----------\n",
    "def load_pdfs_to_texts(pdf_paths):\n",
    "    from pypdf import PdfReader\n",
    "    chunks = []\n",
    "    for p in pdf_paths:\n",
    "        try:\n",
    "            reader = PdfReader(p)\n",
    "            text = \"\\n\".join(page.extract_text() or \"\" for page in reader.pages)\n",
    "            chunks.extend(split_text(text, max_len=1200, overlap=150))\n",
    "        except Exception as e:\n",
    "            print(f\"[RAG] PDF read failed for {p}: {e}\")\n",
    "    return chunks\n",
    "\n",
    "def split_text(txt, max_len=1200, overlap=150):\n",
    "    txt = re.sub(r\"\\s+\", \" \", txt).strip()\n",
    "    out = []\n",
    "    start = 0\n",
    "    while start < len(txt):\n",
    "        end = min(len(txt), start + max_len)\n",
    "        # try to cut on sentence boundary\n",
    "        cut = txt.rfind(\". \", start, end)\n",
    "        if cut == -1 or cut <= start + 200:\n",
    "            cut = end\n",
    "        out.append(txt[start:cut].strip())\n",
    "        start = max(cut - overlap, 0) if cut < len(txt) else len(txt)\n",
    "    return [c for c in out if c]\n",
    "\n",
    "def build_or_load_faiss(chunks, force_rebuild=False):\n",
    "    import faiss\n",
    "    from openai import OpenAI\n",
    "    client = get_openai()\n",
    "    # load existing\n",
    "    if (not force_rebuild) and os.path.exists(INDEX_PATH) and os.path.exists(INDEX_META_PATH):\n",
    "        try:\n",
    "            index = faiss.read_index(INDEX_PATH)\n",
    "            meta = json.load(open(INDEX_META_PATH, \"r\", encoding=\"utf-8\"))\n",
    "            print(f\"[RAG] Loaded existing index with {index.ntotal} vectors.\")\n",
    "            return index, meta\n",
    "        except Exception as e:\n",
    "            print(f\"[RAG] Failed to load index; rebuilding: {e}\")\n",
    "\n",
    "    # embed\n",
    "    print(f\"[RAG] Embedding {len(chunks)} chunks...\")\n",
    "    vecs = []\n",
    "    B = 128\n",
    "    for i in range(0, len(chunks), B):\n",
    "        batch = chunks[i:i+B]\n",
    "        emb = client.embeddings.create(model=EMBED_MODEL, input=batch)\n",
    "        vecs.extend([np.array(e.embedding, dtype=np.float32) for e in emb.data])\n",
    "\n",
    "    d = len(vecs[0]) if vecs else 1536\n",
    "    index = faiss.IndexFlatIP(d)\n",
    "    faiss.normalize_L2(np.asarray(vecs))\n",
    "    index.add(np.asarray(vecs))\n",
    "    faiss.write_index(index, INDEX_PATH)\n",
    "    json.dump({\"chunks\": chunks}, open(INDEX_META_PATH, \"w\", encoding=\"utf-8\"))\n",
    "    print(f\"[RAG] Indexed {len(chunks)} chunks.\")\n",
    "    return index, {\"chunks\": chunks}\n",
    "\n",
    "def vector_retrieve(query, index, meta, k=3):\n",
    "    if index is None or meta is None:\n",
    "        return []\n",
    "    from openai import OpenAI\n",
    "    client = get_openai()\n",
    "    emb = client.embeddings.create(model=EMBED_MODEL, input=[query]).data[0].embedding\n",
    "    q = np.array(emb, dtype=np.float32)[None, :]\n",
    "    import faiss\n",
    "    faiss.normalize_L2(q)\n",
    "    D, I = index.search(q, k)\n",
    "    chunks = [meta[\"chunks\"][i] for i in I[0] if 0 <= i < len(meta[\"chunks\"])]\n",
    "    return [{\"title\": \"PDF KB\", \"text\": c[:200], \"docId\": f\"kb-{i}\"} for i, c in zip(I[0], chunks)]\n",
    "\n",
    "# --------- LLM orchestration ----------\n",
    "def llm_answer(user_text: str, snippets):\n",
    "    from openai import OpenAI\n",
    "    client = get_openai()\n",
    "    ctx = \"\\n\".join(f\"- {s['title']}: {s['text']}\" for s in snippets)\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": f\"{SYSTEM_PROMPT}\\n\\nUse the following context to answer the user's question:\\n{ctx}\"},\n",
    "        {\"role\": \"user\", \"content\": user_text}\n",
    "    ]\n",
    "    resp = client.chat.completions.create(\n",
    "        model=\"gpt-3.5-turbo\",\n",
    "        messages=messages,\n",
    "        temperature=0.3,\n",
    "        max_tokens=150\n",
    "    )\n",
    "    return resp.choices[0].message.content.strip()\n",
    "\n",
    "def llm_stream_sentences(user_text: str, snippets):\n",
    "    from openai import OpenAI\n",
    "    client = get_openai()\n",
    "    ctx = \"\\n\".join(f\"- {s['title']}: {s['text']}\" for s in snippets)\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": f\"{SYSTEM_PROMPT}\\n\\nUse the following context to answer the user's question:\\n{ctx}\"},\n",
    "        {\"role\": \"user\", \"content\": user_text}\n",
    "    ]\n",
    "    buffer = \"\"\n",
    "    try:\n",
    "        stream = client.chat.completions.create(\n",
    "            model=\"gpt-3.5-turbo\",\n",
    "            messages=messages,\n",
    "            temperature=0.3,\n",
    "            max_tokens=150,\n",
    "            stream=True\n",
    "        )\n",
    "        for chunk in stream:\n",
    "            token = getattr(getattr(chunk.choices[0], \"delta\", None), \"content\", None)\n",
    "            if not token: \n",
    "                continue\n",
    "            buffer += token\n",
    "            parts = re.split(r\"(?<=[.!?])\\s+\", buffer)\n",
    "            for sent in parts[:-1]:\n",
    "                s = sent.strip()\n",
    "                if s:\n",
    "                    yield s\n",
    "            buffer = parts[-1]\n",
    "        if buffer.strip():\n",
    "            yield buffer.strip()\n",
    "    except Exception:\n",
    "        yield llm_answer(user_text, snippets)\n",
    "\n",
    "# --------- OpenAI TTS (WAV) ----------\n",
    "def wav_bytes_to_np(wav_bytes: bytes):\n",
    "    with wave.open(io.BytesIO(wav_bytes), \"rb\") as wf:\n",
    "        sr = wf.getframerate()\n",
    "        nframes = wf.getnframes()\n",
    "        audio = wf.readframes(nframes)\n",
    "        sampwidth = wf.getsampwidth()\n",
    "        if sampwidth != 2:\n",
    "            # convert to 16-bit if needed\n",
    "            dtype = {1: np.int8, 2: np.int16, 3: None, 4: np.int32}.get(sampwidth, None)\n",
    "            if dtype is None:\n",
    "                raise RuntimeError(f\"Unsupported sample width: {sampwidth}\")\n",
    "        pcm16 = np.frombuffer(audio, dtype=np.int16)\n",
    "    return pcm16, sr\n",
    "\n",
    "def soften_punctuation(text: str):\n",
    "    # Reduce hard stops to limit pauses (don’t remove the final one)\n",
    "    text = re.sub(r\"(?<!\\.)\\.(\\s+)\", r\", \", text)   # interior periods -> commas\n",
    "    text = re.sub(r\";\", \",\", text)\n",
    "    return text\n",
    "\n",
    "def coalesce_sentences(sentences, max_chars=350):\n",
    "    # Group sentences so each TTS call has a natural paragraph\n",
    "    acc, acc_len = [], 0\n",
    "    out = []\n",
    "    for s in sentences:\n",
    "        s = s.strip()\n",
    "        if not s:\n",
    "            continue\n",
    "        if acc_len + len(s) + 1 > max_chars and acc:\n",
    "            out.append(\" \".join(acc))\n",
    "            acc, acc_len = [s], len(s)\n",
    "        else:\n",
    "            acc.append(s); acc_len += len(s) + 1\n",
    "    if acc:\n",
    "        out.append(\" \".join(acc))\n",
    "    return out\n",
    "\n",
    "def tts_openai(text: str):\n",
    "    # Request WAV bytes from OpenAI TTS (fixes the RIFF error)\n",
    "    if not text:\n",
    "        return np.zeros(0, dtype=np.int16), SAMPLE_RATE\n",
    "    from openai import OpenAI\n",
    "    client = get_openai()\n",
    "\n",
    "    text = soften_punctuation(text)\n",
    "    # Non-streaming is simpler and yields fewer artifacts for short prompts\n",
    "    audio = client.audio.speech.create(\n",
    "        model=OPENAI_TTS_MODEL,\n",
    "        voice=VOICE_NAME,\n",
    "        input=text,\n",
    "        response_format=\"wav\"   # <---- IMPORTANT: not 'format'\n",
    "    )\n",
    "    wav_bytes = audio.content if hasattr(audio, \"content\") else audio  # SDK returns a Response with .content\n",
    "    if isinstance(wav_bytes, bytes) is False:\n",
    "        # Some SDK versions return a dict-like; get .content_bytes()\n",
    "        wav_bytes = audio.get(\"content\") if isinstance(audio, dict) else bytes(audio)\n",
    "    pcm16, sr = wav_bytes_to_np(wav_bytes)\n",
    "    return pcm16, sr\n",
    "\n",
    "# --------- IO ----------\n",
    "class AudioIO:\n",
    "    def __init__(self):\n",
    "        self.sr = SAMPLE_RATE\n",
    "    def mic_frames(self):\n",
    "        frames = int(self.sr * FRAME_MS / 1000)\n",
    "        with sd.InputStream(samplerate=self.sr, channels=1, dtype='int16', blocksize=frames) as stream:\n",
    "            while True:\n",
    "                data, _ = stream.read(frames)\n",
    "                yield data.flatten()\n",
    "    def play(self, pcm16: np.ndarray, sr: int | None = None):\n",
    "        if pcm16.size == 0:\n",
    "            return\n",
    "        sd.play(pcm16, sr if sr else self.sr)\n",
    "        sd.wait()\n",
    "\n",
    "# --------- Speak pipeline (with dedupe & fewer pauses) ----------\n",
    "def sentence_split(text: str):\n",
    "    return [s.strip() for s in re.split(r\"(?<=[.!?])\\s+\", text or \"\") if s.strip()]\n",
    "\n",
    "def speak_sentences(sentences, allow_contact: bool, dedupe_ai: RecentDeduper, ioa: AudioIO):\n",
    "    # Filter + coalesce, then one TTS per paragraph for smoother prosody\n",
    "    filtered = []\n",
    "    seen_keys = set()\n",
    "    for sent in sentences:\n",
    "        if not sent:\n",
    "            continue\n",
    "        if not allow_contact and contains_contact_info(sent):\n",
    "            continue\n",
    "        key = normalize_for_dedupe(sent)\n",
    "        if not key or key in seen_keys:\n",
    "            continue\n",
    "        if dedupe_ai.seen(sent, within=10.0):\n",
    "            continue\n",
    "        seen_keys.add(key)\n",
    "        filtered.append(sent)\n",
    "\n",
    "    if not filtered:\n",
    "        return\n",
    "\n",
    "    # Group into paragraphs to reduce awkward gaps\n",
    "    paragraphs = coalesce_sentences(filtered, max_chars=350)\n",
    "    print(f\"AI: {' '.join(paragraphs)}\")\n",
    "    for p in paragraphs:\n",
    "        audio, sr = tts_openai(p)\n",
    "        ioa.play(audio, sr)\n",
    "\n",
    "# --------- RAG glue ----------\n",
    "FAISS_INDEX = None\n",
    "FAISS_META = None\n",
    "\n",
    "def build_kb(pdf_paths=None, extra_texts=None, force_rebuild=False):\n",
    "    global FAISS_INDEX, FAISS_META\n",
    "    pdf_paths = pdf_paths or []\n",
    "    extra_texts = extra_texts or []\n",
    "    txts = []\n",
    "    if pdf_paths:\n",
    "        txts += load_pdfs_to_texts(pdf_paths)\n",
    "    for t in extra_texts:\n",
    "        txts += split_text(t, max_len=1200, overlap=150)\n",
    "    if not txts:\n",
    "        FAISS_INDEX, FAISS_META = None, None\n",
    "        print(\"[RAG] No KB provided.\")\n",
    "        return\n",
    "    FAISS_INDEX, FAISS_META = build_or_load_faiss(txts, force_rebuild=force_rebuild)\n",
    "\n",
    "def retrieve_context(query: str, k_seed=2, k_vec=3):\n",
    "    snippets = kw_rag(query, k=k_seed)\n",
    "    vec_snips = vector_retrieve(query, FAISS_INDEX, FAISS_META, k=k_vec) if FAISS_INDEX else []\n",
    "    # Remove contact unless asked\n",
    "    allow_contact = contact_intent(query)\n",
    "    all_snips = snippets + vec_snips\n",
    "    if not allow_contact:\n",
    "        all_snips = [s for s in all_snips if \"contact\" not in s[\"title\"].lower()]\n",
    "    return all_snips\n",
    "\n",
    "def process_text(text: str, ioa: AudioIO, dedupe_ai: RecentDeduper):\n",
    "    allow_contact = contact_intent(text)\n",
    "    # build context from both seed & vector KB\n",
    "    snippets = retrieve_context(text)\n",
    "    # Stream sentences, but we’ll coalesce before TTS\n",
    "    speak_sentences(llm_stream_sentences(text, snippets), allow_contact, dedupe_ai, ioa)\n",
    "\n",
    "# --------- Main loop ----------\n",
    "def main(pdf_paths=None, extra_texts=None, force_rebuild=False):\n",
    "    # Optional: build RAG from PDFs & extra text\n",
    "    if pdf_paths or extra_texts:\n",
    "        build_kb(pdf_paths=pdf_paths, extra_texts=extra_texts, force_rebuild=force_rebuild)\n",
    "\n",
    "    init_asr()\n",
    "    ioa = AudioIO()\n",
    "    vad = VAD()\n",
    "    dedupe_user = RecentDeduper()\n",
    "    dedupe_ai = RecentDeduper()\n",
    "\n",
    "    greeting = \"Welcome to Ahmed Group. How can I assist you today?\"\n",
    "    print(f\"AI: {greeting}\")\n",
    "    g_pcm, g_sr = tts_openai(greeting)\n",
    "    ioa.play(g_pcm, g_sr)\n",
    "\n",
    "    buf = []\n",
    "    last_speech = time.time()\n",
    "\n",
    "    try:\n",
    "        for frame in ioa.mic_frames():\n",
    "            if vad.is_speech(frame):\n",
    "                buf.append(frame)\n",
    "                last_speech = time.time()\n",
    "                continue\n",
    "\n",
    "            if buf and (time.time() - last_speech) > UTTER_TIMEOUT:\n",
    "                pcm = np.concatenate(buf)\n",
    "                buf = []\n",
    "                duration = len(pcm) / SAMPLE_RATE\n",
    "                if duration < MIN_UTTER_SEC or duration > MAX_AUDIO_BUFFER_SEC:\n",
    "                    continue\n",
    "                text = transcribe(pcm)\n",
    "                if not text:\n",
    "                    continue\n",
    "                if dedupe_user.seen(text, within=6.0):\n",
    "                    continue\n",
    "                print(f\"User: {text}\")\n",
    "                process_text(text, ioa, dedupe_ai)\n",
    "    except KeyboardInterrupt:\n",
    "        print(\"\\nStopped\")\n",
    "\n",
    "# --------- Entrypoint ----------\n",
    "if globals().get(\"_VOICE_CONCIERGE_RUNNING\"):\n",
    "    print(\"(Already running — ignoring duplicate start)\")\n",
    "else:\n",
    "    globals()[\"_VOICE_CONCIERGE_RUNNING\"] = True\n",
    "    try:\n",
    "        # Example: point these to your 4 PDFs; add any extra knowledge text blobs\n",
    "        pdfs = [\n",
    "            \"/home/kxngh/Desktop/GenieAI/AIcallerV2/kb/Exempt Market Proficiency Course-EMP.pdf\",\n",
    "            \"/home/kxngh/Desktop/GenieAI/AIcallerV2/kb/Tawakkul Fund Deck.pdf\",\n",
    "            \"/home/kxngh/Desktop/GenieAI/AIcallerV2/kb/Unregistered Principal Training Program.pdf\",\n",
    "            \"kb/v5.3 - AGPT Deck.pdf\",\n",
    "        ]\n",
    "        extra_blobs = [\n",
    "            # \"Short policy note ...\",\n",
    "            # \"FAQ entry ...\",\n",
    "        ]\n",
    "        main(pdf_paths=pdfs, extra_texts=extra_blobs, force_rebuild=False)\n",
    "    finally:\n",
    "        globals()[\"_VOICE_CONCIERGE_RUNNING\"] = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "777ed710",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kxngh/Desktop/GenieAI/AIcallerV2/.venv/lib64/python3.11/site-packages/webrtcvad.py:1: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
      "  import pkg_resources\n",
      "/home/kxngh/Desktop/GenieAI/AIcallerV2/.venv/lib64/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m<frozen importlib._bootstrap_external>:1440\u001b[39m, in \u001b[36m_path_importer_cache\u001b[39m\u001b[34m(cls, path)\u001b[39m\n",
      "\u001b[31mKeyError\u001b[39m: '/home/kxngh/Desktop/GenieAI/AIcallerV2/.venv/lib64/python3.11/site-packages/faiss'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 12\u001b[39m\n\u001b[32m      9\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mfaster_whisper\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m WhisperModel\n\u001b[32m     11\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpypdf\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m PdfReader\n\u001b[32m---> \u001b[39m\u001b[32m12\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mfaiss\u001b[39;00m\n\u001b[32m     13\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mrank_bm25\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m BM25Okapi\n\u001b[32m     15\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnltk\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/GenieAI/AIcallerV2/.venv/lib64/python3.11/site-packages/faiss/__init__.py:17\u001b[39m\n\u001b[32m     14\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01minspect\u001b[39;00m\n\u001b[32m     16\u001b[39m \u001b[38;5;66;03m# We import * so that the symbol foo can be accessed as faiss.foo.\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m17\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mloader\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m *\n\u001b[32m     19\u001b[39m \u001b[38;5;66;03m# additional wrappers\u001b[39;00m\n\u001b[32m     20\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mfaiss\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m class_wrappers\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<frozen importlib._bootstrap>:1176\u001b[39m, in \u001b[36m_find_and_load\u001b[39m\u001b[34m(name, import_)\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<frozen importlib._bootstrap>:1138\u001b[39m, in \u001b[36m_find_and_load_unlocked\u001b[39m\u001b[34m(name, import_)\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<frozen importlib._bootstrap>:1078\u001b[39m, in \u001b[36m_find_spec\u001b[39m\u001b[34m(name, path, target)\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<frozen importlib._bootstrap_external>:1507\u001b[39m, in \u001b[36mfind_spec\u001b[39m\u001b[34m(cls, fullname, path, target)\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<frozen importlib._bootstrap_external>:1476\u001b[39m, in \u001b[36m_get_spec\u001b[39m\u001b[34m(cls, fullname, path, target)\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<frozen importlib._bootstrap_external>:1442\u001b[39m, in \u001b[36m_path_importer_cache\u001b[39m\u001b[34m(cls, path)\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<frozen importlib._bootstrap_external>:1418\u001b[39m, in \u001b[36m_path_hooks\u001b[39m\u001b[34m(path)\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<frozen importlib._bootstrap_external>:1701\u001b[39m, in \u001b[36mpath_hook_for_FileFinder\u001b[39m\u001b[34m(path)\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<frozen importlib._bootstrap_external>:168\u001b[39m, in \u001b[36m_path_isdir\u001b[39m\u001b[34m(path)\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<frozen importlib._bootstrap_external>:153\u001b[39m, in \u001b[36m_path_is_mode_type\u001b[39m\u001b[34m(path, mode)\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<frozen importlib._bootstrap_external>:147\u001b[39m, in \u001b[36m_path_stat\u001b[39m\u001b[34m(path)\u001b[39m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "import os, io, time, re, json, shutil, threading, wave\n",
    "from pathlib import Path\n",
    "from collections import deque\n",
    "\n",
    "import numpy as np\n",
    "import sounddevice as sd\n",
    "import webrtcvad\n",
    "from dotenv import load_dotenv\n",
    "from faster_whisper import WhisperModel\n",
    "\n",
    "from pypdf import PdfReader\n",
    "import faiss\n",
    "from rank_bm25 import BM25Okapi\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "\n",
    "# NLTK bootstrap\n",
    "def ensure_nltk_resources():\n",
    "    resources = {\n",
    "        \"punkt\": \"tokenizers/punkt\",\n",
    "        \"punkt_tab\": \"tokenizers/punkt_tab/english.pickle\",\n",
    "    }\n",
    "    for pkg, path in resources.items():\n",
    "        try:\n",
    "            nltk.data.find(path)\n",
    "        except LookupError:\n",
    "            try:\n",
    "                nltk.download(pkg, quiet=True)\n",
    "            except Exception as err:\n",
    "                print(f\"[NLTK] failed to download {pkg}: {err}\")\n",
    "\n",
    "ensure_nltk_resources()\n",
    "\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "VALID_VOICES = {\n",
    "    \"alloy\",\"echo\",\"fable\",\"onyx\",\"nova\",\"shimmer\",\n",
    "    \"coral\",\"verse\",\"ballad\",\"ash\",\"sage\",\"marin\",\"cedar\"\n",
    "}\n",
    "\n",
    "def _sanitize_env_enum(var_name: str, default: str, allowed: set[str]) -> str:\n",
    "    raw = os.getenv(var_name, default) or default\n",
    "    token = re.split(r\"[#,;]\", raw, maxsplit=1)[0]\n",
    "    token = token.strip().strip('\"').strip(\"'\").replace(\"”\",\"\").replace(\"“\",\"\").replace(\"’\",\"\").replace(\"‘\",\"\")\n",
    "    token_l = token.lower()\n",
    "    if token_l not in allowed:\n",
    "        print(f\"[TTS] Warning: {var_name}='{raw}' sanitized -> '{token_l}', invalid. Using '{default}'.\")\n",
    "        return default\n",
    "    return token_l\n",
    "\n",
    "\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "OPENAI_TTS_MODEL = os.getenv(\"OPENAI_TTS_MODEL\", \"gpt-4o-mini-tts\")\n",
    "VOICE_NAME = _sanitize_env_enum(\"OPENAI_TTS_VOICE\", \"verse\", VALID_VOICES)\n",
    "\n",
    "WHISPER_MODEL_NAME = os.getenv(\"WHISPER_MODEL_NAME\", \"tiny.en\")\n",
    "WHISPER_COMPUTE_TYPE = os.getenv(\"WHISPER_COMPUTE_TYPE\", \"int8\")\n",
    "\n",
    "SAMPLE_RATE = 16000\n",
    "FRAME_MS = 10\n",
    "UTTER_TIMEOUT = 0.4\n",
    "MIN_UTTER_SEC = 0.5\n",
    "MAX_AUDIO_BUFFER_SEC = 10\n",
    "\n",
    "\n",
    "SYSTEM_PROMPT = (\n",
    "    \"You are Ahmed Group Concierge. Be professional and brief (1–2 sentences). \"\n",
    ")\n",
    "\n",
    "INVESTMENT_DISCLAIMER = (\n",
    "    \"This is not an offer to sell or a solicitation to buy securities. \"\n",
    "    \"No performance projections or returns are offered.\"\n",
    ")\n",
    "\n",
    "CMS_QA = {\n",
    "    \"about\":\n",
    "        \"Ahmed Group is a Canadian family office and real-estate firm (est. 1966).\",\n",
    "    \"agpt\":\n",
    "        \"AGPT is available for accredited investors. \" + INVESTMENT_DISCLAIMER,\n",
    "    \"dundas\":\n",
    "        \"Ahmed Group reached a settlement on May 21, 2024 to advance a 568-unit rental development.\",\n",
    "    \"vendor\":\n",
    "        \"Please review our Vendor Guidelines and current tenders. I can also collect your details.\",\n",
    "    \"location\":\n",
    "        \"Corporate Head Office: 1-1024 Dundas St. E., Mississauga, ON L4Y 2B8.\",\n",
    "    \"resident_script\":\n",
    "        \"We’re active in the Mississauga community, including senior residences. What’s your ideal move-in window?\",\n",
    "    \"media_script\":\n",
    "        \"Ahmed Group reached a settlement on May 21, 2024 regarding the Dundas project. What’s your outlet and deadline?\",\n",
    "}\n",
    "\n",
    "SEED_SNIPPETS = [\n",
    "    {\"id\": \"about-001\", \"title\": \"Company Overview\",\n",
    "     \"text\": \"Ahmed Group is a Canadian family office and real-estate firm founded in 1966.\"},\n",
    "    {\"id\": \"agpt-2025-08-28\", \"title\": \"AG Property Trust\",\n",
    "     \"text\": \"AGPT is a private real estate investment trust for accredited investors. \" + INVESTMENT_DISCLAIMER},\n",
    "    {\"id\": \"dundas-2024-05-21\", \"title\": \"Dundas St. E. Project\",\n",
    "     \"text\": \"A 568-unit rental development advanced after the May 21, 2024 settlement.\"},\n",
    "    {\"id\": \"contact-001\", \"title\": \"Contact / HQ\",\n",
    "     \"text\": \"Corporate Head Office: 1-1024 Dundas St. E., Mississauga, ON L4Y 2B8.\"},\n",
    "]\n",
    "\n",
    "\n",
    "CONTACT_KEYWORDS = [\"contact\",\"phone\",\"email\",\"reach\",\"call\",\"address\",\"location\",\"where are you\",\"hq\",\"head office\",\"based\"]\n",
    "FAREWELL_KEYWORDS = [\n",
    "    \"thank you that's all\",\"thanks that's all\",\"that's all thank you\",\"that's all thanks\",\n",
    "    \"that's everything\",\"that'll be all\",\"that will be all\",\"i'm good thank you\",\n",
    "    \"i'm all set\",\"that's it thank you\",\"that's it thanks\"\n",
    "]\n",
    "\n",
    "EMAIL_RE = re.compile(r\"\\b[\\w\\.-]+@[\\w\\.-]+\\.[A-Za-z]{2,}\\b\")\n",
    "PHONE_RE = re.compile(r\"\\+?\\d[\\d\\s\\-()]{7,}\\d\")\n",
    "ADDRESS_HINTS = [\"dundas st\", \"l4y\", \"mississauga\", \"1-1024 dundas\"]\n",
    "\n",
    "\n",
    "def get_openai():\n",
    "    if not OPENAI_API_KEY:\n",
    "        raise RuntimeError(\"OPENAI_API_KEY not set\")\n",
    "    from openai import OpenAI\n",
    "    return OpenAI(api_key=OPENAI_API_KEY)\n",
    "\n",
    "\n",
    "def contact_intent(text: str) -> bool:\n",
    "    t = (text or \"\").lower()\n",
    "    return any(k in t for k in CONTACT_KEYWORDS)\n",
    "\n",
    "\n",
    "def farewell_intent(text: str) -> bool:\n",
    "    t = (text or \"\").lower().strip()\n",
    "    if any(phrase in t for phrase in FAREWELL_KEYWORDS):\n",
    "        return True\n",
    "    return len(t.split()) <= 6 and \"thank\" in t and \"all\" in t\n",
    "\n",
    "\n",
    "def contains_contact_info(s: str) -> bool:\n",
    "    ss = s.lower()\n",
    "    return EMAIL_RE.search(s) or PHONE_RE.search(s) or any(h in ss for h in ADDRESS_HINTS)\n",
    "\n",
    "\n",
    "def normalize_for_dedupe(s: str) -> str:\n",
    "    if not isinstance(s, str): return \"\"\n",
    "    t = s.strip().lower()\n",
    "    t = t.replace(\"\\u2013\", \"-\").replace(\"\\u2014\", \"-\")\n",
    "    t = re.sub(r\"\\s+\", \" \", t)\n",
    "    t = re.sub(r\"[^a-z0-9 ]+\", \"\", t)\n",
    "    return t.strip()\n",
    "\n",
    "\n",
    "class RecentDeduper:\n",
    "    def __init__(self, max_items: int = 64):\n",
    "        self.buf = deque()\n",
    "        self.max_items = max_items\n",
    "\n",
    "    def seen(self, s: str, within: float = 8.0) -> bool:\n",
    "        now = time.time()\n",
    "        key = normalize_for_dedupe(s)\n",
    "\n",
    "        while self.buf and (now - self.buf[0][1]) > within:\n",
    "            self.buf.popleft()\n",
    "\n",
    "        for txt, ts in self.buf:\n",
    "            if txt == key:\n",
    "                return True\n",
    "\n",
    "        self.buf.append((key, now))\n",
    "        if len(self.buf) > self.max_items:\n",
    "            self.buf.popleft()\n",
    "        return False\n",
    "\n",
    "\n",
    "class VAD:\n",
    "    def __init__(self):\n",
    "        self.vad = webrtcvad.Vad(1)\n",
    "        self.sr = SAMPLE_RATE\n",
    "        self.noise_gate = 400\n",
    "\n",
    "    def is_speech(self, pcm16: np.ndarray) -> bool:\n",
    "        if np.abs(pcm16).max() < self.noise_gate:\n",
    "            return False\n",
    "        try:\n",
    "            return self.vad.is_speech(pcm16.tobytes(), self.sr)\n",
    "        except:\n",
    "            return False\n",
    "\n",
    "\n",
    "\n",
    "# RAG (FAISS + BM25)\n",
    "\n",
    "RAG_DIR = Path(\"./rag_index\")\n",
    "EMBED_MODEL = \"text-embedding-3-small\"\n",
    "INDEX = None\n",
    "INDEX_METADATA = []\n",
    "BM25 = None\n",
    "\n",
    "\n",
    "def _embed_texts(texts):\n",
    "    client = get_openai()\n",
    "    resp = client.embeddings.create(model=EMBED_MODEL, input=texts)\n",
    "    return np.array([d.embedding for d in resp.data], dtype=\"float32\")\n",
    "\n",
    "\n",
    "def _norm_ws(s: str) -> str:\n",
    "    return re.sub(r\"\\s+\", \" \", (s or \"\").strip())\n",
    "\n",
    "\n",
    "def _smart_chunks(text: str, max_chars=900, overlap=120):\n",
    "    paras = [p.strip() for p in re.split(r\"\\n\\s*\\n\", text or \"\") if p.strip()]\n",
    "    chunks, cur = [], \"\"\n",
    "    for p in paras:\n",
    "        if not cur:\n",
    "            cur = p\n",
    "        elif len(cur) + 1 + len(p) <= max_chars:\n",
    "            cur = f\"{cur}\\n{p}\"\n",
    "        else:\n",
    "            chunks.append(_norm_ws(cur))\n",
    "            tail = cur[-overlap:] if overlap else \"\"\n",
    "            cur = f\"{tail}\\n{p}\" if tail else p\n",
    "    if cur.strip():\n",
    "        chunks.append(_norm_ws(cur))\n",
    "    return chunks\n",
    "\n",
    "\n",
    "def _read_pdf(path: str) -> str:\n",
    "    try:\n",
    "        r = PdfReader(path)\n",
    "        out = []\n",
    "        for p in r.pages:\n",
    "            out.append(p.extract_text() or \"\")\n",
    "        return \"\\n\".join(out)\n",
    "    except Exception as e:\n",
    "        print(f\"[RAG] PDF read failed {path}: {e}\")\n",
    "        return \"\"\n",
    "\n",
    "\n",
    "def _ensure_dir(p: Path):\n",
    "    p.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "\n",
    "def _save_index(index, docs):\n",
    "    _ensure_dir(RAG_DIR)\n",
    "    faiss.write_index(index, str(RAG_DIR / \"faiss.index\"))\n",
    "    (RAG_DIR / \"meta.json\").write_text(json.dumps(docs, ensure_ascii=False))\n",
    "    print(f\"[RAG] Saved index with {len(docs)} chunks.\")\n",
    "\n",
    "\n",
    "def _load_index():\n",
    "    global INDEX, INDEX_METADATA, BM25\n",
    "    if not (RAG_DIR / \"faiss.index\").exists():\n",
    "        return False\n",
    "    INDEX = faiss.read_index(str(RAG_DIR / \"faiss.index\"))\n",
    "    INDEX_METADATA = json.loads((RAG_DIR / \"meta.json\").read_text())\n",
    "    corpus_texts = [d[\"text\"] for d in INDEX_METADATA]\n",
    "    tokenized = [word_tokenize(t.lower()) for t in corpus_texts]\n",
    "    BM25 = BM25Okapi(tokenized)\n",
    "    print(f\"[RAG] Loaded index with {len(INDEX_METADATA)} chunks.\")\n",
    "    return True\n",
    "\n",
    "\n",
    "def reset_index():\n",
    "    if RAG_DIR.exists():\n",
    "        shutil.rmtree(RAG_DIR)\n",
    "    print(\"[RAG] Reset index directory.\")\n",
    "\n",
    "\n",
    "def load_knowledge(pdf_paths=None, extra_text_files=None, rebuild=False):\n",
    "    global INDEX, INDEX_METADATA, BM25\n",
    "\n",
    "    if not rebuild and _load_index():\n",
    "        return\n",
    "\n",
    "    pdf_paths = pdf_paths or []\n",
    "    extra_text_files = extra_text_files or []\n",
    "    docs = []\n",
    "\n",
    "    for s in SEED_SNIPPETS:\n",
    "        for i, ch in enumerate(_smart_chunks(s[\"text\"])):\n",
    "            docs.append({\"id\": f\"seed::{s['id']}::{i}\", \"title\": s[\"title\"], \"text\": ch})\n",
    "\n",
    "    for p in pdf_paths:\n",
    "        txt = _read_pdf(p)\n",
    "        if not txt:\n",
    "            continue\n",
    "        for i, chunk in enumerate(_smart_chunks(txt, max_chars=1100, overlap=160)):\n",
    "            docs.append({\"id\": f\"pdf::{os.path.basename(p)}::{i}\", \"title\": os.path.basename(p), \"text\": chunk})\n",
    "\n",
    "    for tpath in extra_text_files:\n",
    "        try:\n",
    "            raw = Path(tpath).read_text(encoding=\"utf-8\")\n",
    "            for i, chunk in enumerate(_smart_chunks(raw, max_chars=1100, overlap=160)):\n",
    "                docs.append({\"id\": f\"text::{os.path.basename(tpath)}::{i}\", \"title\": os.path.basename(tpath), \"text\": chunk})\n",
    "        except Exception as e:\n",
    "            print(f\"[RAG] text read failed {tpath}: {e}\")\n",
    "\n",
    "    if not docs:\n",
    "        INDEX, INDEX_METADATA, BM25 = None, [], None\n",
    "        print(\"[RAG] No docs to index.\")\n",
    "        return\n",
    "\n",
    "    texts = [d[\"text\"] for d in docs]\n",
    "    embs = _embed_texts(texts).astype(\"float32\")\n",
    "    dim = embs.shape[1]\n",
    "    faiss.normalize_L2(embs)\n",
    "    index = faiss.IndexFlatIP(dim)\n",
    "    index.add(embs)\n",
    "    INDEX = index\n",
    "    INDEX_METADATA = docs\n",
    "\n",
    "    tokenized = [word_tokenize(t.lower()) for t in texts]\n",
    "    BM25 = BM25Okapi(tokenized)\n",
    "    _save_index(INDEX, INDEX_METADATA)\n",
    "\n",
    "\n",
    "def _mmr(query_emb, cand_embs, lambda_mult=0.55, top_k=5):\n",
    "    sim = np.dot(cand_embs, query_emb.T).reshape(-1)\n",
    "    selected_idx = []\n",
    "    selected = []\n",
    "\n",
    "    while len(selected_idx) < min(top_k, len(sim)):\n",
    "        if not selected_idx:\n",
    "            i = int(np.argmax(sim))\n",
    "            selected_idx.append(i)\n",
    "            selected.append(cand_embs[i])\n",
    "            continue\n",
    "\n",
    "        rem_idx = [i for i in range(len(sim)) if i not in selected_idx]\n",
    "        div = []\n",
    "\n",
    "        for i in rem_idx:\n",
    "            max_sim = max(np.dot(cand_embs[i], s) for s in selected)\n",
    "            score = lambda_mult * sim[i] - (1 - lambda_mult) * max_sim\n",
    "            div.append((score, i))\n",
    "\n",
    "        i = max(div, key=lambda x: x[0])[1]\n",
    "        selected_idx.append(i)\n",
    "        selected.append(cand_embs[i])\n",
    "\n",
    "    return selected_idx\n",
    "\n",
    "\n",
    "def _hybrid_search(query: str, k=6, allow_contact=False, expand=False):\n",
    "    q_emb = _embed_texts([query]).astype(\"float32\")\n",
    "    faiss.normalize_L2(q_emb)\n",
    "\n",
    "    oversample = min(50, len(INDEX_METADATA))\n",
    "    D, I = INDEX.search(q_emb, oversample)\n",
    "    cand_idxs = list(dict.fromkeys(I[0].tolist()))\n",
    "\n",
    "    bm25_scores = np.zeros(len(INDEX_METADATA))\n",
    "    if BM25:\n",
    "        toks = word_tokenize(query.lower())\n",
    "        s = np.array(BM25.get_scores(toks))\n",
    "        bm25_scores = np.maximum(bm25_scores, s)\n",
    "\n",
    "    fused = []\n",
    "    for i in cand_idxs:\n",
    "        meta = INDEX_METADATA[i]\n",
    "        if not allow_contact and \"contact\" in meta[\"title\"].lower():\n",
    "            continue\n",
    "        kw_score = bm25_scores[i]\n",
    "        combined = 0.65 + 0.35 * (kw_score / (kw_score + 3.0 + 1e-9))\n",
    "        fused.append((i, combined))\n",
    "\n",
    "    if not fused:\n",
    "        return []\n",
    "\n",
    "    fused.sort(key=lambda x: x[1], reverse=True)\n",
    "    top_idx = [i for i, _ in fused[:min(24, len(fused))]]\n",
    "\n",
    "    cand_embs = _embed_texts([INDEX_METADATA[i][\"text\"] for i in top_idx]).astype(\"float32\")\n",
    "    faiss.normalize_L2(cand_embs)\n",
    "    picked = _mmr(q_emb[0], cand_embs, lambda_mult=0.55, top_k=k)\n",
    "    final_idx = [top_idx[i] for i in picked]\n",
    "\n",
    "    return [{\"title\": INDEX_METADATA[i][\"title\"],\n",
    "             \"text\": INDEX_METADATA[i][\"text\"],\n",
    "             \"docId\": INDEX_METADATA[i][\"id\"],\n",
    "             \"rank\": r+1}\n",
    "            for r, i in enumerate(final_idx)]\n",
    "\n",
    "\n",
    "def rag_search(query: str, k=4, allow_contact=False):\n",
    "    if INDEX is None or INDEX.ntotal == 0:\n",
    "        return kw_rag_keyword(query, k=k, allow_contact=allow_contact)\n",
    "    return _hybrid_search(query, k=k, allow_contact=allow_contact)\n",
    "\n",
    "\n",
    "def kw_rag_keyword(query: str, k: int = 2, allow_contact=False):\n",
    "    q = query.lower().split()\n",
    "    docs = list(SEED_SNIPPETS)\n",
    "    if not allow_contact:\n",
    "        docs = [d for d in docs if \"contact\" not in d[\"title\"].lower()]\n",
    "\n",
    "    scored = [(sum(w in d[\"text\"].lower() for w in q), i, d)\n",
    "              for i, d in enumerate(docs)]\n",
    "    scored.sort(reverse=True, key=lambda x: x[0])\n",
    "    return [{\"title\": d[\"title\"], \"text\": d[\"text\"][:400], \"docId\": d[\"id\"]}\n",
    "            for _, i, d in scored[:k]]\n",
    "\n",
    "\n",
    "def cms_match(user_text: str):\n",
    "    norm = (user_text or \"\").lower()\n",
    "\n",
    "    def has(words): return any(w in norm for w in words)\n",
    "\n",
    "    if farewell_intent(norm):\n",
    "        return \"Thank you for contacting Ahmed Group.\"\n",
    "\n",
    "    if has([\"agpt\",\"accredited investor\",\"accredited\",\"ag trust\",\"reit\"]):\n",
    "        return CMS_QA[\"agpt\"]\n",
    "\n",
    "    if has([\"dundas\",\"1000\",\"1024\",\"mother parkers\"]):\n",
    "        return CMS_QA[\"dundas\"]\n",
    "\n",
    "    if has([\"vendor\",\"procure\",\"tender\",\"bid\"]):\n",
    "        return CMS_QA[\"vendor\"]\n",
    "\n",
    "    if has([\"senior\",\"residence\",\"the pearl\"]):\n",
    "        return CMS_QA[\"resident_script\"]\n",
    "\n",
    "    if has([\"media\",\"press\",\"journalist\",\"reporter\"]):\n",
    "        return CMS_QA[\"media_script\"]\n",
    "\n",
    "    if has(CONTACT_KEYWORDS):\n",
    "        return CMS_QA[\"location\"]\n",
    "\n",
    "    if has([\"what do you do\",\"about\",\"company\",\"overview\",\"who are you\"]):\n",
    "        return CMS_QA[\"about\"]\n",
    "\n",
    "    return None\n",
    "\n",
    "\n",
    "ASR = None\n",
    "def init_asr():\n",
    "    global ASR\n",
    "    print(f\"Loading Whisper {WHISPER_MODEL_NAME} on CPU...\")\n",
    "    ASR = WhisperModel(WHISPER_MODEL_NAME, device=\"cpu\",\n",
    "                       compute_type=WHISPER_COMPUTE_TYPE, num_workers=2)\n",
    "\n",
    "\n",
    "def transcribe(pcm16: np.ndarray) -> str:\n",
    "    audio = pcm16.astype(np.float32) / 32768.0\n",
    "    segments, _ = ASR.transcribe(audio, language=\"en\", beam_size=1)\n",
    "    return \" \".join((s.text or \"\").strip() for s in segments).strip()\n",
    "\n",
    "\n",
    "def _build_context(snippets):\n",
    "    return \"\\n\".join(f\"- {s['title']}: {s['text']}\" for s in snippets)\n",
    "\n",
    "\n",
    "def llm_generate(user_text: str, snippets, allow_contact=False) -> str:\n",
    "    client = get_openai()\n",
    "    ctx = _build_context(snippets)\n",
    "\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
    "        {\"role\": \"user\", \"content\": f\"Context:\\n{ctx}\\n\\nUser question: {user_text}\"}\n",
    "    ]\n",
    "\n",
    "    resp = client.chat.completions.create(\n",
    "        model=\"gpt-4o-mini\",\n",
    "        messages=messages,\n",
    "        temperature=0.2,\n",
    "        max_tokens=220\n",
    "    )\n",
    "\n",
    "    out = (resp.choices[0].message.content or \"\").strip()\n",
    "\n",
    "    if not allow_contact:\n",
    "        out = EMAIL_RE.sub(\"\", out)\n",
    "        out = PHONE_RE.sub(\"\", out)\n",
    "        for hint in ADDRESS_HINTS:\n",
    "            out = re.sub(re.escape(hint), \"\", out, flags=re.I)\n",
    "        out = re.sub(r\"\\s{2,}\", \" \", out).strip()\n",
    "\n",
    "    return out\n",
    "\n",
    "\n",
    "# TTS\n",
    "\n",
    "def tts_openai(text: str):\n",
    "    return tts_openai_wav_only(text)\n",
    "\n",
    "\n",
    "def tts_openai_wav_only(text: str):\n",
    "    try:\n",
    "        if not text:\n",
    "            return np.zeros(0, dtype=np.int16), SAMPLE_RATE\n",
    "\n",
    "        client = get_openai()\n",
    "        use_voice = VOICE_NAME if VOICE_NAME in VALID_VOICES else \"verse\"\n",
    "\n",
    "        resp = client.audio.speech.create(\n",
    "            model=OPENAI_TTS_MODEL,\n",
    "            voice=use_voice,\n",
    "            input=text,\n",
    "            response_format=\"wav\",\n",
    "        )\n",
    "\n",
    "        wav_bytes = getattr(resp, \"content\", None)\n",
    "        if not wav_bytes:\n",
    "            try: wav_bytes = resp.read()\n",
    "            except: pass\n",
    "\n",
    "        if not wav_bytes:\n",
    "            print(\"[TTS] Empty response\")\n",
    "            return np.zeros(0, dtype=np.int16), SAMPLE_RATE\n",
    "\n",
    "        with wave.open(io.BytesIO(wav_bytes), \"rb\") as wf:\n",
    "            sr = wf.getframerate()\n",
    "            ch = wf.getnchannels()\n",
    "            sw = wf.getsampwidth()\n",
    "            frames = wf.readframes(wf.getnframes())\n",
    "\n",
    "            pcm = np.frombuffer(frames, dtype=np.int16)\n",
    "            if ch > 1:\n",
    "                pcm = pcm.reshape(-1, ch).mean(axis=1).astype(np.int16)\n",
    "\n",
    "        if sr != SAMPLE_RATE:\n",
    "            duration = pcm.size / float(sr)\n",
    "            target_len = max(int(round(duration * SAMPLE_RATE)), 1)\n",
    "            x_old = np.linspace(0.0, 1.0, pcm.size, endpoint=False, dtype=np.float32)\n",
    "            x_new = np.linspace(0.0, 1.0, target_len, endpoint=False, dtype=np.float32)\n",
    "            resampled = np.interp(x_new, x_old, pcm.astype(np.float32))\n",
    "            pcm16 = np.clip(resampled, -32768, 32767).astype(np.int16)\n",
    "        else:\n",
    "            pcm16 = pcm.astype(np.int16)\n",
    "\n",
    "        print(f\"[TTS] ok: samples={len(pcm16)} sr={SAMPLE_RATE}\")\n",
    "        return pcm16, SAMPLE_RATE\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"[TTS] Fatal error: {e}\")\n",
    "        return np.zeros(0, dtype=np.int16), SAMPLE_RATE\n",
    "\n",
    "\n",
    "class AudioPlayer:\n",
    "    def __init__(self, sr=SAMPLE_RATE):\n",
    "        self.sr = sr\n",
    "        self._lock = threading.Lock()\n",
    "        self._is_playing = False\n",
    "        self._play_thread = None\n",
    "\n",
    "    def is_playing(self):\n",
    "        with self._lock:\n",
    "            return self._is_playing\n",
    "\n",
    "    def _play_blocking(self, pcm16: np.ndarray):\n",
    "        if pcm16.size == 0:\n",
    "            with self._lock: self._is_playing = False\n",
    "            return\n",
    "\n",
    "        with self._lock:\n",
    "            self._is_playing = True\n",
    "\n",
    "        try:\n",
    "            sd.play(pcm16, self.sr)\n",
    "            sd.wait()\n",
    "        except Exception as e:\n",
    "            print(f\"[Audio] Playback error: {e}\")\n",
    "\n",
    "        with self._lock:\n",
    "            self._is_playing = False\n",
    "\n",
    "    def play_async(self, pcm16: np.ndarray):\n",
    "        t = threading.Thread(target=self._play_blocking, args=(pcm16,), daemon=True)\n",
    "        self._play_thread = t\n",
    "        t.start()\n",
    "\n",
    "\n",
    "def speak_text_one_shot(text: str, allow_contact: bool, player: AudioPlayer):\n",
    "    if not text:\n",
    "        return\n",
    "    if not allow_contact and contains_contact_info(text):\n",
    "        lines = [ln for ln in text.splitlines() if not contains_contact_info(ln)]\n",
    "        text = \" \".join(lines).strip()\n",
    "\n",
    "    audio, _ = tts_openai(text)\n",
    "    print(f\"AI: {text}\")\n",
    "    player.play_async(audio)\n",
    "\n",
    "\n",
    "def process_text(text: str, player: AudioPlayer):\n",
    "    allow_contact = contact_intent(text)\n",
    "    cms = cms_match(text)\n",
    "    if cms:\n",
    "        return speak_text_one_shot(cms, allow_contact, player)\n",
    "\n",
    "    snippets = rag_search(text, k=4, allow_contact=allow_contact)\n",
    "    answer = llm_generate(text, snippets, allow_contact=allow_contact)\n",
    "    return speak_text_one_shot(answer, allow_contact, player)\n",
    "\n",
    "\n",
    "def main():\n",
    "    load_knowledge(pdf_paths=[], extra_text_files=[], rebuild=False)\n",
    "    init_asr()\n",
    "\n",
    "    class AudioIO:\n",
    "        def __init__(self): self.sr = SAMPLE_RATE\n",
    "        def mic_frames(self):\n",
    "            frames = int(self.sr * FRAME_MS / 1000)\n",
    "            with sd.InputStream(samplerate=self.sr, channels=1, dtype='int16', blocksize=frames) as stream:\n",
    "                while True:\n",
    "                    data, _ = stream.read(frames)\n",
    "                    yield data.flatten()\n",
    "\n",
    "    ioa = AudioIO()\n",
    "    vad = VAD()\n",
    "    player = AudioPlayer(sr=ioa.sr)\n",
    "\n",
    "    greeting = \"Welcome to Ahmed Group. How can I assist you today?\"\n",
    "    print(f\"AI: {greeting}\")\n",
    "    g_audio, _ = tts_openai(greeting)\n",
    "    print(f\"[TTS] Greeting samples: {len(g_audio)}\")\n",
    "    player.play_async(g_audio)\n",
    "\n",
    "    while player.is_playing():\n",
    "        time.sleep(0.01)\n",
    "\n",
    "    buf = []\n",
    "    last_speech = time.time()\n",
    "\n",
    "    try:\n",
    "        for frame in ioa.mic_frames():\n",
    "            if vad.is_speech(frame):\n",
    "                buf.append(frame)\n",
    "                last_speech = time.time()\n",
    "                continue\n",
    "\n",
    "            if buf and (time.time() - last_speech) > UTTER_TIMEOUT:\n",
    "                pcm = np.concatenate(buf)\n",
    "                buf = []\n",
    "                duration = len(pcm) / SAMPLE_RATE\n",
    "\n",
    "                if duration < MIN_UTTER_SEC or duration > MAX_AUDIO_BUFFER_SEC:\n",
    "                    continue\n",
    "\n",
    "                text = transcribe(pcm)\n",
    "                if not text:\n",
    "                    continue\n",
    "\n",
    "                print(f\"User: {text}\")\n",
    "                process_text(text, player)\n",
    "\n",
    "    except KeyboardInterrupt:\n",
    "        print(\"\\nStopped\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    if globals().get(\"_VOICE_CONCIERGE_RUNNING\", False):\n",
    "        print(\"(Already running — ignoring duplicate start)\")\n",
    "    else:\n",
    "        globals()[\"_VOICE_CONCIERGE_RUNNING\"] = True\n",
    "        try:\n",
    "            main()\n",
    "        finally:\n",
    "            globals()[\"_VOICE_CONCIERGE_RUNNING\"] = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "206776be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[RAG] Loaded index with 4 chunks.\n",
      "Loading Whisper tiny.en on CPU (int8)...\n",
      "AI: Welcome to Ahmed Group. How can I assist you today?\n",
      "[TTS] Realtime fallback failed: This event loop is already running\n",
      "[TTS] Greeting samples: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_28547/3397696603.py:591: RuntimeWarning: coroutine '_realtime_tts_request' was never awaited\n",
      "  return np.zeros(0, dtype=np.int16), SAMPLE_RATE\n",
      "RuntimeWarning: Enable tracemalloc to get the object allocation traceback\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Stopped\n"
     ]
    }
   ],
   "source": [
    "import os, io, time, re, json, shutil, threading, wave, asyncio, base64, ssl\n",
    "from pathlib import Path\n",
    "from collections import deque\n",
    "\n",
    "import numpy as np\n",
    "import sounddevice as sd\n",
    "import webrtcvad\n",
    "from dotenv import load_dotenv\n",
    "from faster_whisper import WhisperModel\n",
    "\n",
    "# RAG deps\n",
    "from pypdf import PdfReader\n",
    "import faiss\n",
    "\n",
    "# RAG extras\n",
    "from rank_bm25 import BM25Okapi\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# -------------------- NLTK bootstrap --------------------\n",
    "def ensure_nltk_resources():\n",
    "    resources = {\n",
    "        \"punkt\": \"tokenizers/punkt\",\n",
    "        \"punkt_tab\": \"tokenizers/punkt_tab/english.pickle\",\n",
    "    }\n",
    "    for pkg, path in resources.items():\n",
    "        try:\n",
    "            nltk.data.find(path)\n",
    "        except LookupError:\n",
    "            try:\n",
    "                nltk.download(pkg, quiet=True)\n",
    "            except Exception as err:\n",
    "                print(f\"[NLTK] failed to download {pkg}: {err}\")\n",
    "\n",
    "ensure_nltk_resources()\n",
    "\n",
    "# -------------------- ENV & CONFIG --------------------\n",
    "load_dotenv()\n",
    "\n",
    "VALID_VOICES = {\n",
    "    \"alloy\",\"echo\",\"fable\",\"onyx\",\"nova\",\"shimmer\",\n",
    "    \"coral\",\"verse\",\"ballad\",\"ash\",\"sage\",\"marin\",\"cedar\"\n",
    "}\n",
    "\n",
    "def _sanitize_env_enum(var_name: str, default: str, allowed: set[str]) -> str:\n",
    "    raw = os.getenv(var_name, default) or default\n",
    "    token = re.split(r\"[#,;]\", raw, maxsplit=1)[0]\n",
    "    token = token.strip().strip('\"').strip(\"'\").replace(\"”\",\"\").replace(\"“\",\"\").replace(\"’\",\"\").replace(\"‘\",\"\")\n",
    "    token_l = token.lower()\n",
    "    if token_l not in allowed:\n",
    "        print(f\"[TTS] Warning: {var_name}='{raw}' sanitized -> '{token_l}', invalid. Falling back to '{default}'.\")\n",
    "        return default\n",
    "    return token_l\n",
    "\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "VOICE_NAME = _sanitize_env_enum(\"OPENAI_TTS_VOICE\", \"verse\", VALID_VOICES)\n",
    "\n",
    "# New: Realtime model + URL (override via env if you like)\n",
    "OPENAI_REALTIME_MODEL = os.getenv(\"OPENAI_REALTIME_MODEL\", \"gpt-realtime\")\n",
    "OPENAI_REALTIME_WS = os.getenv(\n",
    "    \"OPENAI_REALTIME_WS\",\n",
    "    f\"wss://api.openai.com/v1/realtime?model={OPENAI_REALTIME_MODEL}\"\n",
    ")\n",
    "\n",
    "# CPU-only Whisper\n",
    "WHISPER_MODEL_NAME = os.getenv(\"WHISPER_MODEL_NAME\", \"tiny.en\")\n",
    "WHISPER_COMPUTE_TYPE = os.getenv(\"WHISPER_COMPUTE_TYPE\", \"int8\")\n",
    "\n",
    "# Audio IO\n",
    "SAMPLE_RATE = 16000\n",
    "FRAME_MS = 10\n",
    "UTTER_TIMEOUT = 0.4\n",
    "MIN_UTTER_SEC = 0.5\n",
    "MAX_AUDIO_BUFFER_SEC = 10\n",
    "\n",
    "# -------------------- Ahmed Group content --------------------\n",
    "SYSTEM_PROMPT = (\n",
    "    \"You are Ahmed Group Concierge. Be professional and brief (1–2 sentences). \"\n",
    "    \"Only provide contact details if the user explicitly asks for contact information or how to reach Ahmed Group.\"\n",
    ")\n",
    "\n",
    "INVESTMENT_DISCLAIMER = (\n",
    "    \"This is not an offer to sell or a solicitation to buy securities. \"\n",
    "    \"No performance projections or returns are offered. Qualified investors will receive official documents directly from Investor Relations.\"\n",
    ")\n",
    "\n",
    "CMS_QA = {\n",
    "    \"about\":\n",
    "        \"Ahmed Group is a Canadian family office and real-estate firm (est. 1966) focused on discovering and developing purpose-built rental and community-enhancing projects. We manage the full stack: development, asset management, property management, and joint ventures.\",\n",
    "    \"agpt\":\n",
    "        \"Ahmed Group has launched AG Property Trust (AGPT) for accredited investors. I can share an overview and connect you with Investor Relations to provide official documents. Are you an accredited investor, and what’s your preferred email? \"\n",
    "        + INVESTMENT_DISCLAIMER,\n",
    "    \"dundas\":\n",
    "        \"Ahmed Group reached a settlement on May 21, 2024 to advance a 568-unit mixed-use, purpose-built rental at 1000 & 1024 Dundas St. E., Mississauga.\",\n",
    "    \"vendor\":\n",
    "        \"Please review our Vendor Guidelines and current Tenders/Bidding. I can also collect your details for Procurement.\",\n",
    "    \"location\":\n",
    "        \"Corporate Head Office: 1-1024 Dundas St. E., Mississauga, ON L4Y 2B8 | (905) 949-0999 | contact@ahmed.group.\",\n",
    "    \"resident_script\":\n",
    "        \"We’re active in the Mississauga community, including Mississauga Senior Residences and The Pearl. I can arrange a call-back or add you to a waitlist. What’s your ideal move-in window?\",\n",
    "    \"media_script\":\n",
    "        \"Yes—Ahmed Group reached a settlement on May 21, 2024 to advance a 568-unit mixed-use, purpose-built rental at 1000 & 1024 Dundas St. E. I can route you to our communications team—what’s your outlet and deadline?\",\n",
    "}\n",
    "\n",
    "SEED_SNIPPETS = [\n",
    "    {\"id\": \"about-001\", \"title\": \"Company Overview\",\n",
    "     \"text\": \"Ahmed Group is a Canadian family office and real-estate firm (est. 1966) focused on discovering and developing purpose-built rental and community-enhancing projects. We manage development, asset management, property management, and joint ventures.\"},\n",
    "    {\"id\": \"agpt-2025-08-28\", \"title\": \"AG Property Trust\",\n",
    "     \"text\": \"AGPT is a private real estate investment trust launched Aug 28, 2025, now open to accredited investors. \" + INVESTMENT_DISCLAIMER},\n",
    "    {\"id\": \"dundas-2024-05-21\", \"title\": \"Dundas St. E. Project\",\n",
    "     \"text\": \"A 568-unit mixed-use, purpose-built rental at 1000 & 1024 Dundas St. E., Mississauga advanced after a May 21, 2024 settlement with Mother Parkers.\"},\n",
    "    {\"id\": \"contact-001\", \"title\": \"Contact / HQ\",\n",
    "     \"text\": \"Corporate Head Office: 1-1024 Dundas St. E., Mississauga, ON L4Y 2B8 | (905) 949-0999 | contact@ahmed.group.\"},\n",
    "]\n",
    "\n",
    "CONTACT_KEYWORDS = [\"contact\",\"phone\",\"email\",\"reach\",\"call\",\"address\",\"location\",\"where are you\",\"hq\",\"head office\",\"based\"]\n",
    "FAREWELL_KEYWORDS = [\"thank you that's all\",\"thanks that's all\",\"that's all thank you\",\"that's all thanks\",\n",
    "                     \"that's everything\",\"that'll be all\",\"that will be all\",\"i'm good thank you\",\n",
    "                     \"i'm all set\",\"that's it thank you\",\"that's it thanks\"]\n",
    "\n",
    "EMAIL_RE = re.compile(r\"\\b[\\w\\.-]+@[\\w\\.-]+\\.[A-Za-z]{2,}\\b\")\n",
    "PHONE_RE = re.compile(r\"\\+?\\d[\\d\\s\\-()]{7,}\\d\")\n",
    "ADDRESS_HINTS = [\"dundas st\", \"l4y\", \"mississauga\", \"1-1024 dundas\"]\n",
    "\n",
    "# -------------------- OpenAI client (for embeddings + chat) --------------------\n",
    "def get_openai():\n",
    "    if not OPENAI_API_KEY:\n",
    "        raise RuntimeError(\"OPENAI_API_KEY not set. Put it in .env or export it in the shell.\")\n",
    "    from openai import OpenAI\n",
    "    return OpenAI(api_key=OPENAI_API_KEY)\n",
    "\n",
    "# -------------------- Intent helpers --------------------\n",
    "def contact_intent(text: str) -> bool:\n",
    "    t = (text or \"\").lower()\n",
    "    return any(k in t for k in CONTACT_KEYWORDS)\n",
    "\n",
    "def farewell_intent(text: str) -> bool:\n",
    "    t = (text or \"\").lower().strip()\n",
    "    if any(phrase in t for phrase in FAREWELL_KEYWORDS):\n",
    "        return True\n",
    "    if len(t.split()) <= 6 and \"thank\" in t and \"all\" in t:\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "def contains_contact_info(s: str) -> bool:\n",
    "    ss = s.lower()\n",
    "    return EMAIL_RE.search(s) or PHONE_RE.search(s) or any(h in ss for h in ADDRESS_HINTS)\n",
    "\n",
    "# -------------------- Dedupe --------------------\n",
    "def normalize_for_dedupe(s: str) -> str:\n",
    "    if not isinstance(s, str):\n",
    "        return \"\"\n",
    "    t = s.strip().lower()\n",
    "    t = t.replace(\"\\u2013\", \"-\").replace(\"\\u2014\", \"-\")\n",
    "    t = re.sub(r\"\\s+\", \" \", t)\n",
    "    t = re.sub(r\"[^a-z0-9 ]+\", \"\", t)\n",
    "    return t.strip()\n",
    "\n",
    "class RecentDeduper:\n",
    "    def __init__(self, max_items: int = 64):\n",
    "        self.buf = deque()\n",
    "        self.max_items = max_items\n",
    "    def _norm(self, s: str) -> str:\n",
    "        return normalize_for_dedupe(s)\n",
    "    def seen(self, s: str, within: float = 8.0) -> bool:\n",
    "        now = time.time()\n",
    "        key = self._norm(s)\n",
    "        while self.buf and (now - self.buf[0][1]) > within:\n",
    "            self.buf.popleft()\n",
    "        for txt, ts in self.buf:\n",
    "            if txt == key:\n",
    "                return True\n",
    "        self.buf.append((key, now))\n",
    "        if len(self.buf) > self.max_items:\n",
    "            self.buf.popleft()\n",
    "        return False\n",
    "\n",
    "# -------------------- VAD (for utterance end detection only) --------------------\n",
    "class VAD:\n",
    "    def __init__(self):\n",
    "        self.vad = webrtcvad.Vad(1)\n",
    "        self.sr = SAMPLE_RATE\n",
    "        self.noise_gate = 400\n",
    "    def is_speech(self, pcm16: np.ndarray) -> bool:\n",
    "        if np.abs(pcm16).max() < self.noise_gate:\n",
    "            return False\n",
    "        try:\n",
    "            return self.vad.is_speech(pcm16.tobytes(), self.sr)\n",
    "        except:\n",
    "            return False\n",
    "\n",
    "# -------------------- RAG (FAISS+BM25) --------------------\n",
    "RAG_DIR = Path(\"./rag_index\")\n",
    "EMBED_MODEL = \"text-embedding-3-small\"\n",
    "INDEX = None\n",
    "INDEX_METADATA = []\n",
    "BM25 = None\n",
    "\n",
    "def _embed_texts(texts):\n",
    "    client = get_openai()\n",
    "    resp = client.embeddings.create(model=EMBED_MODEL, input=texts)\n",
    "    return np.array([d.embedding for d in resp.data], dtype=\"float32\")\n",
    "\n",
    "def _norm_ws(s: str) -> str:\n",
    "    return re.sub(r\"\\s+\", \" \", (s or \"\").strip())\n",
    "\n",
    "def _smart_chunks(text: str, max_chars=900, overlap=120):\n",
    "    paras = [p.strip() for p in re.split(r\"\\n\\s*\\n\", text or \"\") if p.strip()]\n",
    "    chunks, cur = [], \"\"\n",
    "    for p in paras:\n",
    "        if not cur:\n",
    "            cur = p\n",
    "        elif len(cur) + 1 + len(p) <= max_chars:\n",
    "            cur = f\"{cur}\\n{p}\"\n",
    "        else:\n",
    "            chunks.append(_norm_ws(cur))\n",
    "            tail = cur[-overlap:] if overlap else \"\"\n",
    "            cur = f\"{tail}\\n{p}\" if tail else p\n",
    "    if cur.strip():\n",
    "        chunks.append(_norm_ws(cur))\n",
    "    return chunks\n",
    "\n",
    "def _read_pdf(path: str) -> str:\n",
    "    try:\n",
    "        r = PdfReader(path)\n",
    "        out = []\n",
    "        for p in r.pages:\n",
    "            out.append(p.extract_text() or \"\")\n",
    "        return \"\\n\".join(out)\n",
    "    except Exception as e:\n",
    "        print(f\"[RAG] PDF read failed {path}: {e}\")\n",
    "        return \"\"\n",
    "\n",
    "def _ensure_dir(p: Path):\n",
    "    p.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "def _save_index(index, docs):\n",
    "    _ensure_dir(RAG_DIR)\n",
    "    faiss.write_index(index, str(RAG_DIR / \"faiss.index\"))\n",
    "    (RAG_DIR / \"meta.json\").write_text(json.dumps(docs, ensure_ascii=False))\n",
    "    print(f\"[RAG] Saved index with {len(docs)} chunks.\")\n",
    "\n",
    "def _load_index():\n",
    "    global INDEX, INDEX_METADATA, BM25\n",
    "    if not (RAG_DIR / \"faiss.index\").exists():\n",
    "        return False\n",
    "    INDEX = faiss.read_index(str(RAG_DIR / \"faiss.index\"))\n",
    "    INDEX_METADATA = json.loads((RAG_DIR / \"meta.json\").read_text())\n",
    "    corpus_texts = [d[\"text\"] for d in INDEX_METADATA]\n",
    "    tokenized = [word_tokenize(t.lower()) for t in corpus_texts]\n",
    "    BM25 = BM25Okapi(tokenized)\n",
    "    print(f\"[RAG] Loaded index with {len(INDEX_METADATA)} chunks.\")\n",
    "    return True\n",
    "\n",
    "def reset_index():\n",
    "    if RAG_DIR.exists():\n",
    "        shutil.rmtree(RAG_DIR)\n",
    "    print(\"[RAG] Reset index directory.\")\n",
    "\n",
    "def load_knowledge(pdf_paths=None, extra_text_files=None, rebuild=False):\n",
    "    global INDEX, INDEX_METADATA, BM25\n",
    "    if not rebuild and _load_index():\n",
    "        return\n",
    "\n",
    "    pdf_paths = pdf_paths or []\n",
    "    extra_text_files = extra_text_files or []\n",
    "    docs = []\n",
    "\n",
    "    for s in SEED_SNIPPETS:\n",
    "        for i, ch in enumerate(_smart_chunks(s[\"text\"])):\n",
    "            docs.append({\"id\": f\"seed::{s['id']}::{i}\", \"title\": s[\"title\"], \"text\": ch})\n",
    "\n",
    "    for p in pdf_paths:\n",
    "        txt = _read_pdf(p)\n",
    "        if not txt:\n",
    "            continue\n",
    "        for i, chunk in enumerate(_smart_chunks(txt, max_chars=1100, overlap=160)):\n",
    "            docs.append({\"id\": f\"pdf::{os.path.basename(p)}::{i}\", \"title\": os.path.basename(p), \"text\": chunk})\n",
    "\n",
    "    for tpath in (extra_text_files or []):\n",
    "        try:\n",
    "            raw = Path(tpath).read_text(encoding=\"utf-8\")\n",
    "            for i, chunk in enumerate(_smart_chunks(raw, max_chars=1100, overlap=160)):\n",
    "                docs.append({\"id\": f\"text::{os.path.basename(tpath)}::{i}\", \"title\": os.path.basename(tpath), \"text\": chunk})\n",
    "        except Exception as e:\n",
    "            print(f\"[RAG] text read failed {tpath}: {e}\")\n",
    "\n",
    "    if not docs:\n",
    "        INDEX, INDEX_METADATA, BM25 = None, [], None\n",
    "        print(\"[RAG] No docs to index.\")\n",
    "        return\n",
    "\n",
    "    texts = [d[\"text\"] for d in docs]\n",
    "    embs = _embed_texts(texts).astype(\"float32\")\n",
    "    dim = embs.shape[1]\n",
    "    faiss.normalize_L2(embs)\n",
    "    index = faiss.IndexFlatIP(dim)\n",
    "    index.add(embs)\n",
    "    INDEX = index\n",
    "    INDEX_METADATA = docs\n",
    "\n",
    "    tokenized = [word_tokenize(t.lower()) for t in texts]\n",
    "    BM25 = BM25Okapi(tokenized)\n",
    "    _save_index(INDEX, INDEX_METADATA)\n",
    "\n",
    "def _mmr(query_emb, cand_embs, lambda_mult=0.55, top_k=5):\n",
    "    sim = np.dot(cand_embs, query_emb.T).reshape(-1)\n",
    "    selected, selected_idx = [], []\n",
    "    while len(selected_idx) < min(top_k, len(sim)):\n",
    "        if not selected_idx:\n",
    "            i = int(np.argmax(sim))\n",
    "            selected_idx.append(i)\n",
    "            selected.append(cand_embs[i])\n",
    "            continue\n",
    "        rem_idx = [i for i in range(len(sim)) if i not in selected_idx]\n",
    "        div = []\n",
    "        for i in rem_idx:\n",
    "            max_sim_to_sel = max(np.dot(cand_embs[i], s) for s in selected)\n",
    "            score = lambda_mult * sim[i] - (1 - lambda_mult) * max_sim_to_sel\n",
    "            div.append((score, i))\n",
    "        i = max(div, key=lambda x: x[0])[1]\n",
    "        selected_idx.append(i)\n",
    "        selected.append(cand_embs[i])\n",
    "    return selected_idx\n",
    "\n",
    "def _hybrid_search(query: str, k=6, allow_contact=False, expand=False):\n",
    "    q_variants = [query]\n",
    "\n",
    "    bm25_scores = np.zeros(len(INDEX_METADATA))\n",
    "    if BM25:\n",
    "        for q in q_variants:\n",
    "            toks = word_tokenize(q.lower())\n",
    "            s = np.array(BM25.get_scores(toks))\n",
    "            bm25_scores = np.maximum(bm25_scores, s)\n",
    "\n",
    "    q_emb = _embed_texts([query]).astype(\"float32\")\n",
    "    faiss.normalize_L2(q_emb)\n",
    "    oversample = min(50, len(INDEX_METADATA))\n",
    "    D, I = INDEX.search(q_emb, oversample)\n",
    "    I = I[0].tolist()\n",
    "    cand_idxs = list(dict.fromkeys(I))\n",
    "\n",
    "    fused = []\n",
    "    for i in cand_idxs:\n",
    "        meta = INDEX_METADATA[i]\n",
    "        if not allow_contact and (\"contact\" in meta[\"title\"].lower() or \"contact\" in meta[\"id\"].lower()):\n",
    "            continue\n",
    "        emb_score = 1.0\n",
    "        kw_score = bm25_scores[i] if BM25 is not None else 0.0\n",
    "        fused.append((i, 0.65 * emb_score + 0.35 * (kw_score / (kw_score + 3.0 + 1e-9))))\n",
    "\n",
    "    if not fused:\n",
    "        return []\n",
    "\n",
    "    fused.sort(key=lambda x: x[1], reverse=True)\n",
    "    top_idx = [i for i, _ in fused[:min(24, len(fused))]]\n",
    "\n",
    "    cand_embs = _embed_texts([INDEX_METADATA[i][\"text\"] for i in top_idx]).astype(\"float32\")\n",
    "    faiss.normalize_L2(cand_embs)\n",
    "    picked_local = _mmr(q_emb[0], cand_embs, lambda_mult=0.55, top_k=k)\n",
    "    final_idx = [top_idx[i] for i in picked_local]\n",
    "\n",
    "    results = [{\"title\": INDEX_METADATA[i][\"title\"], \"text\": INDEX_METADATA[i][\"text\"], \"docId\": INDEX_METADATA[i][\"id\"], \"rank\": r+1}\n",
    "               for r, i in enumerate(final_idx)]\n",
    "    return results\n",
    "\n",
    "def rag_search(query: str, k=4, allow_contact=False):\n",
    "    if INDEX is None or INDEX.ntotal == 0:\n",
    "        return kw_rag_keyword(query, k=k, allow_contact=allow_contact)\n",
    "    return _hybrid_search(query, k=k, allow_contact=allow_contact, expand=False)\n",
    "\n",
    "def kw_rag_keyword(query: str, k: int = 2, allow_contact=False):\n",
    "    q = query.lower().split()\n",
    "    docs = list(SEED_SNIPPETS)\n",
    "    if not allow_contact:\n",
    "        docs = [d for d in docs if \"contact\" not in d.get(\"title\",\"\").lower() and \"contact\" not in str(d.get(\"id\",\"\")).lower()]\n",
    "    scored = [(sum(w in d[\"text\"].lower() for w in q), i, d) for i, d in enumerate(docs)]\n",
    "    scored.sort(reverse=True, key=lambda x: x[0])\n",
    "    return [{\"title\": d[\"title\"], \"text\": d[\"text\"][:400], \"docId\": d.get(\"id\", str(i))} for _, i, d in scored[:k]]\n",
    "\n",
    "# -------------------- CMS matcher --------------------\n",
    "def cms_match(user_text: str):\n",
    "    text = user_text if isinstance(user_text, str) else \"\"\n",
    "    norm = text.lower()\n",
    "\n",
    "    def any_in(words):\n",
    "        return any(w in norm for w in words)\n",
    "\n",
    "    if farewell_intent(text):\n",
    "        return \"Thank you for contacting Ahmed Group. Feel free to reach out anytime—we're here to help!\"\n",
    "    if any_in([\"agpt\", \"accredited investor\", \"accredited\", \"ag property trust\", \"ag trust\", \"reit\"]):\n",
    "        return CMS_QA.get(\"agpt\")\n",
    "    if any_in([\"dundas\", \"1000\", \"1024\", \"mother parkers\", \"dundas st. e.\", \"dundas street\"]):\n",
    "        return CMS_QA.get(\"dundas\")\n",
    "    if any_in([\"vendor\", \"procure\", \"procurement\", \"tender\", \"bid\", \"bidding\"]):\n",
    "        return CMS_QA.get(\"vendor\")\n",
    "    if any_in([\"senior\", \"residence\", \"residences\", \"the pearl\", \"mississauga senior\"]):\n",
    "        return CMS_QA.get(\"resident_script\")\n",
    "    if any_in([\"media\", \"press\", \"journalist\", \"reporter\", \"interview\", \"statement\"]):\n",
    "        return CMS_QA.get(\"media_script\")\n",
    "    if any_in(CONTACT_KEYWORDS):\n",
    "        return CMS_QA.get(\"location\")\n",
    "    if any_in([\"what do you do\", \"about\", \"company\", \"what is ahmed\", \"who are you\", \"overview\"]):\n",
    "        return CMS_QA.get(\"about\")\n",
    "    return None\n",
    "\n",
    "# -------------------- ASR (CPU-only) --------------------\n",
    "ASR = None\n",
    "def init_asr():\n",
    "    global ASR\n",
    "    print(f\"Loading Whisper {WHISPER_MODEL_NAME} on CPU ({WHISPER_COMPUTE_TYPE})...\")\n",
    "    ASR = WhisperModel(WHISPER_MODEL_NAME, device=\"cpu\", compute_type=WHISPER_COMPUTE_TYPE, num_workers=2)\n",
    "\n",
    "def transcribe(pcm16: np.ndarray) -> str:\n",
    "    audio = pcm16.astype(np.float32) / 32768.0\n",
    "    segments, _ = ASR.transcribe(audio, language=\"en\", beam_size=1)\n",
    "    return \" \".join((s.text or \"\").strip() for s in segments).strip()\n",
    "\n",
    "# -------------------- LLM (no bracketed tags) --------------------\n",
    "def _build_context(snippets):\n",
    "    return \"\\n\".join(f\"- {s['title']}: {s['text']}\" for s in snippets)\n",
    "\n",
    "def llm_generate(user_text: str, snippets, allow_contact=False) -> str:\n",
    "    client = get_openai()\n",
    "    ctx = _build_context(snippets)\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\":\n",
    "         f\"\"\"{SYSTEM_PROMPT}\n",
    "\n",
    "You must:\n",
    "- Answer concisely in 2–3 short sentences.\n",
    "- Only use facts from the provided context; if unknown, say you’re not sure.\n",
    "- Do NOT include any bracketed citations or tags.\n",
    "- Do NOT include contact details unless the user explicitly asked for them.\"\"\"},\n",
    "        {\"role\": \"user\", \"content\": f\"Context:\\n{ctx}\\n\\nUser question: {user_text}\"}\n",
    "    ]\n",
    "    resp = client.chat.completions.create(\n",
    "        model=\"gpt-4o-mini\",\n",
    "        messages=messages,\n",
    "        temperature=0.2,\n",
    "        max_tokens=220\n",
    "    )\n",
    "    out = (resp.choices[0].message.content or \"\").strip()\n",
    "\n",
    "    if not allow_contact:\n",
    "        out = EMAIL_RE.sub(\"\", out)\n",
    "        out = PHONE_RE.sub(\"\", out)\n",
    "        for hint in ADDRESS_HINTS:\n",
    "            out = re.sub(re.escape(hint), \"\", out, flags=re.I)\n",
    "        out = re.sub(r\"\\s{2,}\", \" \", out).strip()\n",
    "    return out\n",
    "\n",
    "# -------------------- TTS via GPT Realtime (WAV; no barge-in) --------------------\n",
    "# Requires: pip install websockets\n",
    "import websockets\n",
    "\n",
    "async def _realtime_tts_request(text: str, voice: str, ws_url: str, api_key: str) -> bytes:\n",
    "    \"\"\"\n",
    "    Open a Realtime WS session, ask for audio (wav) for provided text,\n",
    "    collect streamed audio chunks, return full WAV bytes.\n",
    "    \"\"\"\n",
    "    if not text:\n",
    "        return b\"\"\n",
    "\n",
    "    headers = {\n",
    "        \"Authorization\": f\"Bearer {api_key}\",\n",
    "        \"OpenAI-Beta\": \"realtime=v1\",\n",
    "    }\n",
    "\n",
    "    # Use default SSL context\n",
    "    ssl_context = ssl.create_default_context()\n",
    "\n",
    "    async with websockets.connect(ws_url, extra_headers=headers, ssl=ssl_context, max_size=None) as ws:\n",
    "        # Send the user text into the conversation\n",
    "        await ws.send(json.dumps({\"type\": \"input_text\", \"text\": text}))\n",
    "\n",
    "        # Create a response that returns audio in the selected voice\n",
    "        await ws.send(json.dumps({\n",
    "            \"type\": \"response.create\",\n",
    "            \"response\": {\n",
    "                \"modalities\": [\"audio\"],\n",
    "                \"instructions\": SYSTEM_PROMPT,\n",
    "                \"audio\": {\"voice\": voice, \"format\": \"wav\"}\n",
    "            }\n",
    "        }))\n",
    "\n",
    "        audio_chunks = []\n",
    "        wav_bytes = b\"\"\n",
    "        # Collect until the server says the response is completed\n",
    "        while True:\n",
    "            msg = await ws.recv()\n",
    "            try:\n",
    "                evt = json.loads(msg)\n",
    "            except Exception:\n",
    "                # some servers may send binary pings; ignore\n",
    "                continue\n",
    "\n",
    "            t = evt.get(\"type\")\n",
    "            if t == \"response.audio.delta\":\n",
    "                # base64-encoded audio chunk (wav stream)\n",
    "                delta_b64 = evt.get(\"delta\") or \"\"\n",
    "                if delta_b64:\n",
    "                    try:\n",
    "                        audio_chunks.append(base64.b64decode(delta_b64))\n",
    "                    except Exception:\n",
    "                        pass\n",
    "            elif t in (\"response.completed\", \"response.stop\", \"error\"):\n",
    "                # stitch chunks (many deltas together are valid WAV if server streams full headers once;\n",
    "                # to be safe, just concat—server typically sends a self-contained wav)\n",
    "                wav_bytes = b\"\".join(audio_chunks)\n",
    "                break\n",
    "\n",
    "        return wav_bytes\n",
    "\n",
    "def _wav_bytes_to_pcm16(wav_bytes: bytes, target_sr=SAMPLE_RATE) -> np.ndarray:\n",
    "    if not wav_bytes:\n",
    "        return np.zeros(0, dtype=np.int16)\n",
    "    try:\n",
    "        with wave.open(io.BytesIO(wav_bytes), \"rb\") as wf:\n",
    "            sr = wf.getframerate()\n",
    "            ch = wf.getnchannels()\n",
    "            sw = wf.getsampwidth()\n",
    "            n  = wf.getnframes()\n",
    "            frames = wf.readframes(n)\n",
    "        if sw != 2:\n",
    "            print(f\"[TTS] Unexpected sample width {sw}, coercing to int16.\")\n",
    "        pcm = np.frombuffer(frames, dtype=np.int16)\n",
    "        if ch == 2:\n",
    "            pcm = pcm.reshape(-1, 2).mean(axis=1).astype(np.int16)\n",
    "        elif ch != 1:\n",
    "            try:\n",
    "                pcm = pcm.reshape(-1, ch).mean(axis=1).astype(np.int16)\n",
    "            except Exception:\n",
    "                pcm = pcm.astype(np.int16)\n",
    "\n",
    "        if sr != target_sr:\n",
    "            duration = pcm.size / float(sr)\n",
    "            target_len = max(int(round(duration * target_sr)), 1)\n",
    "            x_old = np.linspace(0.0, 1.0, pcm.size, endpoint=False, dtype=np.float32)\n",
    "            x_new = np.linspace(0.0, 1.0, target_len, endpoint=False, dtype=np.float32)\n",
    "            resampled = np.interp(x_new, x_old, pcm.astype(np.float32))\n",
    "            pcm16 = np.clip(resampled, -32768, 32767).astype(np.int16)\n",
    "        else:\n",
    "            pcm16 = pcm.astype(np.int16, copy=False)\n",
    "        return pcm16\n",
    "    except Exception as e:\n",
    "        print(f\"[TTS] WAV decode error: {e}\")\n",
    "        return np.zeros(0, dtype=np.int16)\n",
    "\n",
    "def tts_openai(text: str):\n",
    "    \"\"\"Public TTS entry (kept same signature).\"\"\"\n",
    "    return tts_openai_realtime_wav(text)\n",
    "\n",
    "def tts_openai_realtime_wav(text: str):\n",
    "    try:\n",
    "        if not text:\n",
    "            return np.zeros(0, dtype=np.int16), SAMPLE_RATE\n",
    "\n",
    "        use_voice = VOICE_NAME if VOICE_NAME in VALID_VOICES else \"verse\"\n",
    "\n",
    "        # Run the async WS call synchronously\n",
    "        wav_bytes = asyncio.run(_realtime_tts_request(\n",
    "            text=text,\n",
    "            voice=use_voice,\n",
    "            ws_url=OPENAI_REALTIME_WS,\n",
    "            api_key=OPENAI_API_KEY\n",
    "        ))\n",
    "\n",
    "        if not wav_bytes:\n",
    "            print(\"[TTS] Empty WAV from Realtime\")\n",
    "            return np.zeros(0, dtype=np.int16), SAMPLE_RATE\n",
    "\n",
    "        pcm16 = _wav_bytes_to_pcm16(wav_bytes, target_sr=SAMPLE_RATE)\n",
    "        print(f\"[TTS/Realtime] ok: samples={len(pcm16)} sr={SAMPLE_RATE}\")\n",
    "        return pcm16, SAMPLE_RATE\n",
    "\n",
    "    except RuntimeError as e:\n",
    "        # In case asyncio loop already running (e.g., in some environments)\n",
    "        try:\n",
    "            loop = asyncio.get_event_loop()\n",
    "            wav_bytes = loop.run_until_complete(_realtime_tts_request(\n",
    "                text=text,\n",
    "                voice=use_voice,\n",
    "                ws_url=OPENAI_REALTIME_WS,\n",
    "                api_key=OPENAI_API_KEY\n",
    "            ))\n",
    "            pcm16 = _wav_bytes_to_pcm16(wav_bytes, target_sr=SAMPLE_RATE)\n",
    "            return pcm16, SAMPLE_RATE\n",
    "        except Exception as e2:\n",
    "            print(f\"[TTS] Realtime fallback failed: {e2}\")\n",
    "            return np.zeros(0, dtype=np.int16), SAMPLE_RATE\n",
    "    except Exception as e:\n",
    "        print(f\"[TTS] Fatal Realtime TTS error: {e}\")\n",
    "        return np.zeros(0, dtype=np.int16), SAMPLE_RATE\n",
    "\n",
    "# -------------------- Audio Player (no interrupts) --------------------\n",
    "class AudioPlayer:\n",
    "    def __init__(self, sr=SAMPLE_RATE):\n",
    "        self.sr = sr\n",
    "        self._lock = threading.Lock()\n",
    "        self._play_thread = None\n",
    "        self._is_playing = False\n",
    "\n",
    "    def is_playing(self):\n",
    "        with self._lock:\n",
    "            return self._is_playing\n",
    "\n",
    "    def _play_blocking(self, pcm16: np.ndarray):\n",
    "        if pcm16.size == 0:\n",
    "            with self._lock:\n",
    "                self._is_playing = False\n",
    "            return\n",
    "        with self._lock:\n",
    "            self._is_playing = True\n",
    "        try:\n",
    "            sd.play(pcm16, self.sr)\n",
    "            sd.wait()  # block until finished\n",
    "        except Exception as e:\n",
    "            print(f\"[Audio] Playback error: {e}\")\n",
    "        with self._lock:\n",
    "            self._is_playing = False\n",
    "\n",
    "    def play_async(self, pcm16: np.ndarray):\n",
    "        # just play; nothing can stop it mid-way (barge-in removed)\n",
    "        t = threading.Thread(target=self._play_blocking, args=(pcm16,), daemon=True)\n",
    "        self._play_thread = t\n",
    "        t.start()\n",
    "\n",
    "# -------------------- Speak (one-shot) --------------------\n",
    "def speak_text_one_shot(text: str, allow_contact: bool, player: AudioPlayer):\n",
    "    if not text:\n",
    "        return\n",
    "    if not allow_contact and contains_contact_info(text):\n",
    "        lines = [ln for ln in text.splitlines() if not contains_contact_info(ln)]\n",
    "        text = \" \".join(lines).strip()\n",
    "    audio, _ = tts_openai(text)\n",
    "    print(f\"AI: {text}\")\n",
    "    player.play_async(audio)\n",
    "\n",
    "# -------------------- Orchestration --------------------\n",
    "def process_text(text: str, player: AudioPlayer):\n",
    "    allow_contact = contact_intent(text)\n",
    "    cms = cms_match(text)\n",
    "    if cms:\n",
    "        return speak_text_one_shot(cms, allow_contact, player)\n",
    "    snippets = rag_search(text, k=4, allow_contact=allow_contact)\n",
    "    answer = llm_generate(text, snippets, allow_contact=allow_contact)\n",
    "    return speak_text_one_shot(answer, allow_contact, player)\n",
    "\n",
    "def main():\n",
    "    load_knowledge(pdf_paths=[], extra_text_files=[], rebuild=False)\n",
    "    init_asr()\n",
    "\n",
    "    class AudioIO:\n",
    "        def __init__(self): self.sr = SAMPLE_RATE\n",
    "        def mic_frames(self):\n",
    "            frames = int(self.sr * FRAME_MS / 1000)\n",
    "            with sd.InputStream(samplerate=self.sr, channels=1, dtype='int16', blocksize=frames) as stream:\n",
    "                while True:\n",
    "                    data, _ = stream.read(frames)\n",
    "                    yield data.flatten()\n",
    "\n",
    "    ioa = AudioIO()\n",
    "    vad = VAD()\n",
    "    player = AudioPlayer(sr=ioa.sr)\n",
    "\n",
    "    # Greeting (plays fully; nothing can interrupt it)\n",
    "    greeting = \"Welcome to Ahmed Group. How can I assist you today?\"\n",
    "    print(f\"AI: {greeting}\")\n",
    "    g_audio, _ = tts_openai(greeting)\n",
    "    print(f\"[TTS] Greeting samples: {len(g_audio)}\")\n",
    "    player.play_async(g_audio)\n",
    "    while player.is_playing():\n",
    "        time.sleep(0.01)\n",
    "\n",
    "    buf = []\n",
    "    last_speech = time.time()\n",
    "\n",
    "    try:\n",
    "        for frame in ioa.mic_frames():\n",
    "            if vad.is_speech(frame):\n",
    "                buf.append(frame)\n",
    "                last_speech = time.time()\n",
    "                continue\n",
    "\n",
    "            if buf and (time.time() - last_speech) > UTTER_TIMEOUT:\n",
    "                pcm = np.concatenate(buf); buf = []\n",
    "                duration = len(pcm) / SAMPLE_RATE\n",
    "                if duration < MIN_UTTER_SEC or duration > MAX_AUDIO_BUFFER_SEC:\n",
    "                    continue\n",
    "                text = transcribe(pcm)\n",
    "                if not text: continue\n",
    "                print(f\"User: {text}\")\n",
    "                process_text(text, player)\n",
    "    except KeyboardInterrupt:\n",
    "        print(\"\\nStopped\")\n",
    "\n",
    "# -------------------- Entry guard --------------------\n",
    "if __name__ == \"__main__\":\n",
    "    if globals().get(\"_VOICE_CONCIERGE_RUNNING\", False):\n",
    "        print(\"(Already running — ignoring duplicate start)\")\n",
    "    else:\n",
    "        globals()[\"_VOICE_CONCIERGE_RUNNING\"] = True\n",
    "        try:\n",
    "            main()\n",
    "        finally:\n",
    "            globals()[\"_VOICE_CONCIERGE_RUNNING\"] = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f2d4e6b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
